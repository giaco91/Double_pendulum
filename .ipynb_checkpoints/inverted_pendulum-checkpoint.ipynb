{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x132215850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stuff we need\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import copy \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import math\n",
    "from routines import *\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentInvertedPendulum:\n",
    "    #initialization of internal state\n",
    "    def __init__(self,m=1,l=1,g=1,max_steps=200,dt=0.005,add_noise=0.01,bottom_up=False,disturb=False):\n",
    "        self.m=m\n",
    "        self.l=l\n",
    "        self.g=g\n",
    "        self.dt=dt\n",
    "        self.steps_left=max_steps\n",
    "        self.reset(add_noise=add_noise)\n",
    "        self.bottom_up=bottom_up\n",
    "        self.disturb=disturb\n",
    "        if disturb:\n",
    "            self.disturb_history=[0]\n",
    "            self.disturb_counter=0\n",
    "        if bottom_up:\n",
    "            self.reset(theta=0+add_noise*(np.random.rand()-0.5))\n",
    "            self.passed=False\n",
    "        else:\n",
    "            self.reset(add_noise=add_noise)\n",
    "\n",
    "    def reset(self,theta=np.pi,theta_d=0,add_noise=0):\n",
    "        self.state=[theta+add_noise*(np.random.rand()-0.5),theta_d+add_noise*(np.random.rand()-0.5)]\n",
    "        self.state_history=[self.state.copy()]\n",
    "        \n",
    "    #returns the current environment's observation to the agent\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    #allows the agent to query the set of actions it can execute\n",
    "    def sample_action(self):\n",
    "        #return self.state[3]+0.1*(np.random.rand()-0.5)\n",
    "        return 1.2\n",
    "    \n",
    "    def get_energy(self,state):\n",
    "        e_kin=self.m/2*(self.l*state[1])**2\n",
    "        e_pot=-self.m*self.g*self.l*np.cos(state[0])\n",
    "        return e_kin,e_pot\n",
    "    \n",
    "    #signals the end of the episode to the agent\n",
    "    def is_done(self):\n",
    "        return self.steps_left==0\n",
    "    \n",
    "    #central piece: handles agents action and returns reward for the action\n",
    "    def action(self,action):\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is over')\n",
    "        self.step(action)\n",
    "        delta=np.abs(self.state[0]%(2*np.pi)-np.pi)\n",
    "        if self.bottom_up:\n",
    "            e_pot,e_kin=self.get_energy(self.state)\n",
    "            e_tot=e_pot+e_kin\n",
    "            if e_tot>1e3/self.m:\n",
    "                #energy constraint:\n",
    "                self.steps_left=0\n",
    "                return 0\n",
    "            if delta<1:\n",
    "                self.passed=True\n",
    "            if self.passed or True:\n",
    "                if delta<np.pi/2:\n",
    "                    self.steps_left -=1\n",
    "                    #return 10-delta**2\n",
    "                    return 1/(delta+1)\n",
    "                else:\n",
    "                    #self.steps_left=0\n",
    "                    self.steps_left -=1\n",
    "                    return 0\n",
    "            else:\n",
    "                self.steps_left -=1\n",
    "                return 1/(delta+1)\n",
    "        else:\n",
    "            if delta<0.9*np.pi:\n",
    "                self.steps_left -=1\n",
    "                return 1/(delta+0.1)\n",
    "            else:\n",
    "                self.steps_left=0\n",
    "                return 0\n",
    "    \n",
    "    def explicite_euler(self,dt,state,F=0):\n",
    "        theta_dd=self.get_theta_dd(state,F)\n",
    "        next_state= [state[0]+dt*state[1],state[1]+dt*theta_dd]\n",
    "        return next_state\n",
    "    \n",
    "    #the decoupled equations of motion\n",
    "    def get_theta_dd(self,state,F=0):\n",
    "        return F/(self.m*self.l)-self.g*np.sin(state[0])/self.l\n",
    "\n",
    "    \n",
    "    #differentail time step using explicite midpoint method\n",
    "    def step(self,F):\n",
    "        if self.disturb:\n",
    "            if self.disturb_counter==0:\n",
    "                r=np.random.rand()\n",
    "                if r>0.98:\n",
    "                    dF=20\n",
    "                    self.disturb_counter=6\n",
    "                elif r<0.02:\n",
    "                    dF=-20\n",
    "                    self.disturb_counter=6\n",
    "                else:\n",
    "                    dF=0\n",
    "                self.disturb_history.append(dF)\n",
    "            else:\n",
    "                self.disturb_counter-=1\n",
    "                self.disturb_history.append(self.disturb_history[-1])\n",
    "            F+=self.disturb_history[-1]\n",
    "                \n",
    "        next_state=self.explicite_euler(self.dt/2,self.state,F)\n",
    "        theta_dd=self.get_theta_dd(next_state,F)\n",
    "        self.state[0]+=self.dt*next_state[1]\n",
    "        self.state[1]+=self.dt*theta_dd\n",
    "        self.state_history.append(self.state.copy())\n",
    "        \n",
    "    def render(self,img_res=1,save_path='trash_figures/inverted_pendulum.avi'):\n",
    "        frames_per_second=20\n",
    "        take_frame_every=int(1/(self.dt*frames_per_second))\n",
    "        frames=[]\n",
    "        h=int(img_res*200)\n",
    "        w=h\n",
    "        x0=int(w/2)\n",
    "        y0=int(h/2)\n",
    "        h_red=int(0.4*h)\n",
    "        L=h_red\n",
    "        d=int(0.02*h)\n",
    "        d=d*self.m**(1/3)\n",
    "        #max_theta2_d=1.2*np.max(np.abs(phase_traject[:,3]))\n",
    "        for i,state_i in enumerate(self.state_history):\n",
    "            if i%5000==0:\n",
    "                print('rendering iteration: '+str(i)+'/'+str(len(self.state_history)))           \n",
    "            if i%take_frame_every==0:\n",
    "                theta=state_i[0]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L*np.sin(theta)\n",
    "                y1=y0+L*np.cos(theta)\n",
    "                #---draw the image ----\n",
    "                img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x1-d,y1-d),(x1+d,y1+d)], fill=(0,0,0), outline=None)\n",
    "                frames.append(img)\n",
    "        cv2_list=self.pil_list_to_cv2(frames)\n",
    "        self.generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "    #calculates the potential and kinetic energy of the two masses at a given state\n",
    "    def get_energy(self,state):\n",
    "        theta=state[0]\n",
    "        theta_d=state[1]\n",
    "        y=self.l*np.cos(theta)\n",
    "        e_pot=-self.m*self.g*y\n",
    "        e_kin=self.m/2*(self.l*theta_d)**2\n",
    "        return e_pot,e_kin\n",
    "    \n",
    "    #used for video converting\n",
    "    def pil_list_to_cv2(self,pil_list):\n",
    "        #converts a list of pil images to a list of cv2 images\n",
    "        png_list=[]\n",
    "        for pil_img in pil_list:\n",
    "            pil_img.save('trash_image.png',format='png')\n",
    "            png_list.append(cv2.imread('trash_image.png'))\n",
    "        os.remove('trash_image.png')\n",
    "        return png_list\n",
    "\n",
    "    def generate_video(self,cv2_list,path='car_race.avi',fps=10): \n",
    "        #makes a video from a given cv2 image list\n",
    "        if len(cv2_list)==0:\n",
    "            raise ValueError('the given png list is empty!')\n",
    "        video_name = path\n",
    "        frame=cv2_list[0] \n",
    "        # setting the frame width, height width \n",
    "        # the width, height of first image \n",
    "        height, width, layers = frame.shape   \n",
    "        video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "        # Appending the images to the video one by one \n",
    "        for cv2_image in cv2_list:  \n",
    "            video.write(cv2_image) \n",
    "        # Deallocating memories taken for window creation \n",
    "        cv2.destroyAllWindows()  \n",
    "        video.release()  # releasing the video generated \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentInvertedDoublePendulum:\n",
    "    #initialization of internal state\n",
    "    def __init__(self,m1=1,m2=1,l1=1,l2=1,g=1,max_steps=200,dt=0.005,add_noise=0.01,bottom_up=False,disturb=False):\n",
    "        self.m1=m1\n",
    "        self.m2=m2\n",
    "        self.l1=l1\n",
    "        self.l2=l2\n",
    "        self.g=g\n",
    "        self.dt=dt\n",
    "        self.actions=[0]\n",
    "        self.steps_left=max_steps\n",
    "        self.bottom_up=bottom_up#true if you want to play the bottom up mode\n",
    "        self.disturb=disturb\n",
    "        if disturb:\n",
    "            self.disturb_history=[0]\n",
    "            self.disturb_counter=0\n",
    "        if bottom_up:\n",
    "            self.passed=False#store if the outer pendulum has managed to surpass abs(np.pi)\n",
    "            noise=add_noise*(np.random.rand(2)-0.5)\n",
    "            self.reset(theta1=0+noise[0],theta2=0+noise[1])\n",
    "        else:\n",
    "            self.reset(add_noise=add_noise)\n",
    "            \n",
    "    def reset(self,theta1=np.pi,theta2=np.pi,theta1_d=0,theta2_d=0,add_noise=0):\n",
    "        noise=add_noise*(np.random.rand(4)-0.5)\n",
    "        noise[2:]=noise[2:]/(2*max(1,add_noise))#we dont want the velocity noise to be too large\n",
    "        self.state=[theta1+noise[0],theta2+noise[1],theta1_d+noise[2],theta2_d+noise[3]]\n",
    "        self.state_history=[self.state.copy()]\n",
    "        \n",
    "    #returns the current environment's observation to the agent\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    #allows the agent to query the set of actions it can execute\n",
    "    def sample_action(self):\n",
    "        #return self.state[3]+0.1*(np.random.rand()-0.5)\n",
    "        return 1.2\n",
    "\n",
    "    #signals the end of the episode to the agent\n",
    "    def is_done(self):\n",
    "        return self.steps_left==0\n",
    "    \n",
    "    #central piece: handles agents action and returns reward for the action\n",
    "    def action(self,action):\n",
    "        self.actions.append(action)\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is over')\n",
    "        self.step(F_1=action,F_2=0)\n",
    "        delta1=np.abs(self.state[1]%(2*np.pi)-np.pi)\n",
    "        #energy constraint:\n",
    "        e_pot,e_kin=self.get_energy(self.state)\n",
    "        e_tot=np.sum(e_pot)+np.sum(e_kin)\n",
    "        if e_tot>1e2/(self.m1+self.m2):\n",
    "            self.steps_left=0\n",
    "            return 0\n",
    "        if self.bottom_up:\n",
    "            if delta1<0.4:\n",
    "                self.passed=True\n",
    "            if self.passed or True:\n",
    "                delta0=np.abs(self.state[0]%(2*np.pi)-np.pi)\n",
    "                if delta1<np.pi/3 and delta0<np.pi/2:\n",
    "                    self.steps_left -=1\n",
    "                    return 1/(delta1+0.05)+1/(delta1+0.5)/(delta0+0.5)\n",
    "                else:\n",
    "                    #self.steps_left=0\n",
    "                    self.steps_left -=1#if you want to play the game until max step reached\n",
    "                    return 0\n",
    "            else:\n",
    "                self.steps_left -=1\n",
    "                return 3-delta1**2\n",
    "        else:\n",
    "            delta0=np.abs(self.state[0]%(2*np.pi)-np.pi)\n",
    "            if (delta1<np.pi/2 and delta0<np.pi/2):\n",
    "                self.steps_left -=1\n",
    "                return 1/(delta1+0.05)+1/(delta1+0.5)/(delta0+0.5)\n",
    "            else:\n",
    "                self.steps_left=0\n",
    "                #self.steps_left -=1#if you want to play the game until max step reached\n",
    "                return 0\n",
    "    \n",
    "    def explicite_euler(self,dt,state,F_1=0,F_2=0):\n",
    "        theta1_dd,theta2_dd=self.get_theta_dd(state,F_1,F_2)\n",
    "        next_state= [state[0]+dt*state[2],state[1]+dt*state[3],state[2]+dt*theta1_dd,state[3]+dt*theta2_dd]\n",
    "        return next_state\n",
    "    \n",
    "    #the decoupled equations of motion\n",
    "    def get_theta_dd(self,state,F_1=0,F_2=0):\n",
    "        theta1=state[0]\n",
    "        theta2=state[1]\n",
    "        theta1_d=state[2]\n",
    "        theta2_d=state[3]\n",
    "        #----theta1_dd-----\n",
    "        num1=-self.g*((2*self.m1+self.m2)*np.sin(theta1)+self.m2*np.sin(theta1-2*theta2))\n",
    "        num2=-2*np.sin(theta1-theta2)*self.m2*(theta2_d**2*self.l2+theta1_d**2*self.l1*np.cos(theta1-theta2))\n",
    "        num3=2*(F_1-F_2*self.m2/(self.m1+self.m2)*np.cos(theta1-theta2))\n",
    "        denum1=2*self.m1+self.m2-self.m2*np.cos(2*theta1-2*theta2)\n",
    "        denum=self.l1*denum1\n",
    "        theta1_dd=(num1+num2+num3)/denum\n",
    "        #----theta2_dd----\n",
    "        num1=2*np.sin(theta1-theta2)\n",
    "        num2=theta1_d**2*self.l1*(self.m1+self.m2)+self.g*(self.m1+self.m2)*np.cos(theta1)+theta2_d**2*self.l2*self.m2*np.cos(theta1-theta2)\n",
    "        num3=2*(F_2-F_1*np.cos(theta1-theta2))\n",
    "        denum=self.l2*denum1\n",
    "        theta2_dd=(num3+num1*num2)/denum\n",
    "        return theta1_dd,theta2_dd\n",
    "    \n",
    "    #differentail time step using explicite midpoint method\n",
    "    def step(self,F_1,F_2):\n",
    "        if self.disturb:\n",
    "            if self.disturb_counter==0:\n",
    "                r=np.random.rand()\n",
    "                if r>0.98:\n",
    "                    dF=5\n",
    "                    self.disturb_counter=6\n",
    "                elif r<0.02:\n",
    "                    dF=-5\n",
    "                    self.disturb_counter=6\n",
    "                else:\n",
    "                    dF=0\n",
    "                self.disturb_history.append(dF)\n",
    "            else:\n",
    "                self.disturb_counter-=1\n",
    "                self.disturb_history.append(self.disturb_history[-1])\n",
    "            F_2+=self.disturb_history[-1]\n",
    "            \n",
    "        next_state=self.explicite_euler(self.dt/2,self.state,F_1,F_2)\n",
    "        theta1_dd,theta2_dd=self.get_theta_dd(next_state,F_1,F_2)\n",
    "        self.state[0]+=self.dt*next_state[2]\n",
    "        self.state[1]+=self.dt*next_state[3]\n",
    "        self.state[2]+=self.dt*theta1_dd\n",
    "        self.state[3]+=self.dt*theta2_dd\n",
    "        self.state_history.append(self.state.copy())\n",
    "        \n",
    "    \n",
    "    def render(self,img_res=1,scores=None,disturb_history=None,save_path='trash_figures/inverted_double_pendulum.avi'):\n",
    "        frames_per_second=20\n",
    "        take_frame_every=int(1/(self.dt*frames_per_second))\n",
    "        frames=[]\n",
    "        h=int(img_res*200)\n",
    "        w=h\n",
    "        x0=int(w/2)\n",
    "        y0=int(h/2)\n",
    "        h_red=int(0.4*h)\n",
    "        l_tot=self.l1+self.l2\n",
    "        l1_ratio=self.l1/l_tot\n",
    "        l2_ratio=self.l2/l_tot\n",
    "        L1=l1_ratio*h_red\n",
    "        L2=l2_ratio*h_red\n",
    "        d=int(0.02*h)\n",
    "        d1=d*self.m1**(1/3)\n",
    "        d2=d*self.m2**(1/3)\n",
    "        d_4=d/4\n",
    "        for i,state_i in enumerate(self.state_history):\n",
    "            if i%5000==0:\n",
    "                print('rendering iteration: '+str(i)+'/'+str(len(self.state_history)))           \n",
    "            if i%take_frame_every==0: \n",
    "                img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                F=self.actions[i]\n",
    "                if F<0:\n",
    "                    F_col=(100,100,255)\n",
    "                else:\n",
    "                    F_col=(255,100,100)\n",
    "                d_F=0.2*np.abs(F)*d\n",
    "                draw.ellipse([(x0-2*d_F,y0-2*d_F),(x0+2*d_F,y0+2*d_F)], fill=F_col, outline=(0,0,0))\n",
    "                if scores is not None:\n",
    "                    score=int(scores[i])\n",
    "                    font = ImageFont.truetype(\"arial.ttf\", int(h/20))\n",
    "                    draw.text((int(0.1*h),int(0.05*h)), 'score: '+str(score)[:6], font=font, fill=(0,0,0))\n",
    "                theta1=state_i[0]\n",
    "                theta2=state_i[1]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L1*np.sin(theta1)\n",
    "                y1=y0+L1*np.cos(theta1)\n",
    "                x2=x1+L2*np.sin(theta2)\n",
    "                y2=y1+L2*np.cos(theta2)\n",
    "                #---draw the image ----\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x1-d1,y1-d1),(x1+d1,y1+d1)], fill=(0,0,0), outline=None)\n",
    "                draw.line([(x1,y1),(x2,y2)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x2-d2,y2-d2),(x2+d2,y2+d2)], fill=(0,0,255), outline=None)\n",
    "                #render perturbations\n",
    "                if disturb_history is not None:\n",
    "                    if disturb_history[i]>0:\n",
    "                        draw.ellipse([(x1-2*d,y1-2*d),(x1+2*d,y1+2*d)], fill=(100,100,255), outline=(0,0,0))\n",
    "                    elif disturb_history[i]<0:\n",
    "                        draw.ellipse([(x1-2*d,y1-2*d),(x1+2*d,y1+2*d)], fill=(255,100,100), outline=(0,0,0))\n",
    "                frames.append(img)\n",
    "        cv2_list=self.pil_list_to_cv2(frames)\n",
    "        self.generate_video(cv2_list,path=save_path,fps=frames_per_second)\n",
    "    \n",
    "    #calculates the potential and kinetic energy of the two masses at a given state\n",
    "    def get_energy(self,state):\n",
    "        theta1=state[0]\n",
    "        theta2=state[1]\n",
    "        theta1_d=state[2]\n",
    "        theta2_d=state[3]\n",
    "        y1=self.l1*np.cos(theta1)\n",
    "        y2=y1+self.l2*np.cos(theta2)\n",
    "        e_pot=np.array([-self.m1*self.g*y1,-self.m2*self.g*y2])\n",
    "        e_kin_1=self.m1/2*(self.l1*theta1_d)**2\n",
    "        e_kin_2=(self.l1*theta1_d)**2\n",
    "        e_kin_2+=(self.l2*theta2_d)**2\n",
    "        e_kin_2+=2*self.l1*self.l2*theta1_d*theta2_d*(np.cos(theta1)*np.cos(theta2)+np.sin(theta1)*np.sin(theta2))\n",
    "        e_kin_2*=self.m2/2\n",
    "        e_kin=np.array([e_kin_1,e_kin_2])\n",
    "        return e_pot,e_kin\n",
    "    \n",
    "    #used for video converting\n",
    "    def pil_list_to_cv2(self,pil_list):\n",
    "        #converts a list of pil images to a list of cv2 images\n",
    "        png_list=[]\n",
    "        for pil_img in pil_list:\n",
    "            pil_img.save('trash_image.png',format='png')\n",
    "            png_list.append(cv2.imread('trash_image.png'))\n",
    "        os.remove('trash_image.png')\n",
    "        return png_list\n",
    "\n",
    "    def generate_video(self,cv2_list,path='car_race.avi',fps=10): \n",
    "        #makes a video from a given cv2 image list\n",
    "        if len(cv2_list)==0:\n",
    "            raise ValueError('the given png list is empty!')\n",
    "        video_name = path\n",
    "        frame=cv2_list[0] \n",
    "        # setting the frame width, height width \n",
    "        # the width, height of first image \n",
    "        height, width, layers = frame.shape   \n",
    "        video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "        # Appending the images to the video one by one \n",
    "        for cv2_image in cv2_list:  \n",
    "            video.write(cv2_image) \n",
    "        # Deallocating memories taken for window creation \n",
    "        cv2.destroyAllWindows()  \n",
    "        video.release()  # releasing the video generated \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAgent:\n",
    "    #initialize the counter for the total reward\n",
    "    def __init__(self,n_neurons,bottom_up=False, use_symmetry=False):\n",
    "        self.total_reward=0.0\n",
    "        self.bottom_up=bottom_up\n",
    "        self.initialize_policy(n_neurons)\n",
    "        self.actions=[0]\n",
    "        self.scores=[0]\n",
    "        self.use_symmetry=use_symmetry\n",
    "            \n",
    "    #accepts the environment instance as an argument and allows the agents to observe and act\n",
    "    def step(self,env):\n",
    "        observation=env.get_observation()\n",
    "        #action = env.sample_action()\n",
    "        action=self.get_action(observation)\n",
    "        reward=env.action(action)\n",
    "        self.total_reward+=reward\n",
    "        self.scores.append(self.total_reward)\n",
    "        \n",
    "    def reset_reward(self):\n",
    "        self.total_reward=0\n",
    "        self.actions=[0]\n",
    "        self.scores=[0]\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        s=state.copy()\n",
    "        F_sign=1\n",
    "        if len(s)==4:\n",
    "            x1=np.sin(s[0])\n",
    "            y1=np.cos(s[0])\n",
    "            x2=np.sin(s[1])\n",
    "            y2=np.cos(s[1])\n",
    "            theta1_d=s[2]/5\n",
    "            theta2_d=s[3]/2\n",
    "            s=[x1,y1,x2,y2,theta1_d,theta2_d]\n",
    "        if self.use_symmetry:\n",
    "            if x1<0:\n",
    "                F_sign=-1\n",
    "                s[0]=-s[0]\n",
    "                s[2]=-s[2]\n",
    "                s[3]=-s[3]\n",
    "                s[4]=-s[4]\n",
    "        with torch.no_grad():\n",
    "            F=F_sign*10*self.policy(torch.FloatTensor(s)).item()\n",
    "        self.actions.append(F)\n",
    "        return F\n",
    "    \n",
    "    def render_action_space(self,savepath,img_res=1,trajectory_list=None,make_video=False):\n",
    "        size=int(200*img_res)\n",
    "        step=2*np.pi/size\n",
    "        if make_video and trajectory_list is None:\n",
    "            print('Warning: Can not make a video because the argument trajectory_list is None' )\n",
    "            make_video=False\n",
    "        if make_video:\n",
    "            frames=[]\n",
    "            frames_per_second=20\n",
    "            dt=0.03\n",
    "            take_frame_every=int(1/(dt*frames_per_second))\n",
    "        np_img=np.zeros((size,size))\n",
    "        img = Image.new(\"RGB\", (size, size), \"white\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        pix = img.load()\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                F=self.get_action([i*step,j*step,0,0])\n",
    "                np_img[i,j]=F\n",
    "        np_img/=(np.max(np_img)-np.min(np_img))\n",
    "        np_img-=np.min(np_img)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                pix[i,j]=(int(np_img[i,j]*255),0,int((1-np_img[i,j])*255))\n",
    "        if make_video:\n",
    "            frames.append(img.copy())\n",
    "        if trajectory_list is not None:\n",
    "            largest_trajectory=get_largest_length(trajectory_list)\n",
    "            r=255*np.random.rand(len(trajectory_list),3)\n",
    "            for i in range(1,largest_trajectory):\n",
    "                for j in range(len(trajectory_list)):\n",
    "                    if len(trajectory_list[j])>i+1:\n",
    "                        p_prev=trajectory_list[j][i-1]\n",
    "                        p_cur=trajectory_list[j][i]\n",
    "                        traj_color=(int(r[j,0]),int(r[j,1]),int(r[j,2]))\n",
    "                        theta1_prev=p_prev[0]%(2*np.pi)\n",
    "                        theta2_prev=p_prev[1]%(2*np.pi)\n",
    "                        theta1_cur=p_cur[0]%(2*np.pi)\n",
    "                        theta2_cur=p_cur[1]%(2*np.pi)\n",
    "                        cord_prev=(theta1_prev/(2*np.pi)*size,theta2_prev/(2*np.pi)*size)\n",
    "                        cord_cur=(theta1_cur/(2*np.pi)*size,theta2_cur/(2*np.pi)*size)\n",
    "                        #pix[int(theta1/(2*np.pi)*size),int(theta2/(2*np.pi)*size)]=traj_color\n",
    "                        draw.line([cord_prev,cord_cur],fill=traj_color,width=1)\n",
    "                if make_video and i%take_frame_every==0:\n",
    "                    frames.append(img.copy())\n",
    "        s_2=int(size/2)\n",
    "        draw.ellipse([(s_2-3,s_2-3),(s_2+3,s_2+3)], fill=(0,0,0), outline=(0,0,0))\n",
    "        img.save(savepath)\n",
    "        if make_video:\n",
    "            print(len(frames))\n",
    "            cv2_list=pil_list_to_cv2(frames)\n",
    "            generate_video(cv2_list,path=savepath+'.avi',fps=frames_per_second)\n",
    "        \n",
    "    def mutate(self,muatation_rate=0.1):\n",
    "        with torch.no_grad():\n",
    "            for param in self.policy.parameters():\n",
    "                param.add_(torch.randn(param.size()) * muatation_rate)\n",
    "        \n",
    "    def initialize_policy(self,n_neurons):\n",
    "        if n_neurons[0]==4:\n",
    "            n_neurons[0]=6\n",
    "        self.policy=self.get_nn(n_neurons)\n",
    "    \n",
    "    def get_nn(self,n_neurons):\n",
    "        neural_network=nn.Sequential()\n",
    "        if len(n_neurons)<2:\n",
    "            raise ValueError('n_neurons must contain at least two entries for in- and output')\n",
    "        depth=len(n_neurons)-2\n",
    "        for i in range(depth):\n",
    "            neural_network.add_module(\"layer\"+str(i),nn.Sequential(nn.Linear(n_neurons[i],n_neurons[i+1]),nn.ReLU()))\n",
    "        neural_network.add_module(\"layer\"+str(depth),nn.Sequential(nn.Linear(n_neurons[depth],n_neurons[depth+1],bias=False)))\n",
    "        return neural_network\n",
    "\n",
    "def get_largest_length(list_of_lists):\n",
    "    l=0\n",
    "    for list in list_of_lists:\n",
    "        if len(list)>l:\n",
    "            l=len(list)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for genetic algorithm\n",
    "def get_first_generation(population_size,agent,n_neurons,mutation_rate=0.1,load_best_agent=False):\n",
    "    if load_best_agent:\n",
    "        agent=pickle.load(open('models/best_agent.pkl', \"rb\" ))\n",
    "    agent_list=[]\n",
    "    for i in range(population_size):\n",
    "        new_agent=copy.deepcopy(agent)\n",
    "        new_agent.reset_reward()\n",
    "        if load_best_agent:\n",
    "            if i>=1:\n",
    "                new_agent.mutate(mutation_rate)\n",
    "        else:\n",
    "            new_agent.initialize_policy(n_neurons)\n",
    "        agent_list.append(new_agent)\n",
    "    return agent_list\n",
    "\n",
    "def get_scores(agent_list,environment):\n",
    "    scores=[]\n",
    "    environments=[]\n",
    "    for agent in agent_list:\n",
    "        env=copy.deepcopy(environment)\n",
    "        while not env.is_done():\n",
    "            agent.step(env)\n",
    "        scores.append(agent.total_reward)\n",
    "        environments.append(env)\n",
    "    return scores,environments\n",
    "\n",
    "def get_next_generation(scores,agent_list,n_survivors,population_size,muatation_rate=0.1,store_best_agent=True):\n",
    "    np_scores=np.asarray(scores)\n",
    "    rank_idx=np.argsort(np_scores)\n",
    "    agent_elite=[]\n",
    "    for i in range(n_survivors):\n",
    "        agent_elite.append(agent_list[rank_idx[-i-1]])\n",
    "    if store_best_agent:\n",
    "        pickle.dump(agent_elite[0],open('models/best_agent.pkl', \"wb\" ))\n",
    "    new_agent_list=[]\n",
    "    for j in range(population_size):\n",
    "        new_agent=copy.deepcopy(agent_elite[j%n_survivors])\n",
    "        if j>=1:\n",
    "            new_agent.mutate(muatation_rate)\n",
    "        new_agent.reset_reward()\n",
    "        new_agent_list.append(new_agent)\n",
    "    return new_agent_list,rank_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_length(list_of_lists):\n",
    "    l=0\n",
    "    for list in list_of_lists:\n",
    "        if len(list)>l:\n",
    "            l=len(list)\n",
    "    return l\n",
    "\n",
    "#functions for video presentaiton\n",
    "def render_single_pendulum(state_history_list,dt,best_idx=None,scores=None,actions=None,m=1,img_res=1.5,disturb_history=None,save_path='trash_figures/inverted_pendulum_generation.avi'):\n",
    "    frames_per_second=20\n",
    "    take_frame_every=int(1/(dt*frames_per_second))\n",
    "    frames=[]\n",
    "    h=int(img_res*200)\n",
    "    if actions is None:\n",
    "        w=h\n",
    "    else:\n",
    "        w=int(1.5*h)\n",
    "        u=int(1.3*h)\n",
    "        v=int(h/2)\n",
    "    x0=int(0.6*h)\n",
    "    y0=int(h/2)\n",
    "    h_red=int(0.4*h)\n",
    "    L=h_red\n",
    "    d=int(0.02*h)\n",
    "    d=d*m**(1/3)\n",
    "    if best_idx is not None:\n",
    "        state_history_list.append(state_history_list[best_idx])\n",
    "    largest_trajectory=get_largest_length(state_history_list)\n",
    "    for i in range(largest_trajectory):\n",
    "        img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        if disturb_history is not None:\n",
    "            if disturb_history[i]>0:\n",
    "                draw.ellipse([(x0-2*d,y0-2*d),(x0+2*d,y0+2*d)], fill=(100,100,255), outline=(0,0,0))\n",
    "            elif disturb_history[i]<0:\n",
    "                draw.ellipse([(x0-2*d,y0-2*d),(x0+2*d,y0+2*d)], fill=(255,100,100), outline=(0,0,0))\n",
    "        for j in range(len(state_history_list)):     \n",
    "            if i%take_frame_every==0 and len(state_history_list[j])>i+1:\n",
    "                if best_idx is not None:\n",
    "                    if j==len(state_history_list)-1:\n",
    "                        color=(255,0,0)\n",
    "                        if actions is not None:\n",
    "                            F=actions[i]\n",
    "                            if F<0:\n",
    "                                F_col=(0,0,255)\n",
    "                            else:\n",
    "                                F_col=(255,0,0)\n",
    "                            draw.line([(u,v),(u,v-int(15*img_res*F))],fill=F_col,width=int(5*img_res))\n",
    "                            font = ImageFont.truetype(\"arial.ttf\", int(h/18))\n",
    "                            draw.text((int(u-0.2*h),int(v-0.01*h)), 'force:', font=font, fill=(0,0,0))\n",
    "                        if scores is not None:\n",
    "                            score=int(scores[i])\n",
    "                            font = ImageFont.truetype(\"arial.ttf\", int(h/20))\n",
    "                            draw.text((int(0.1*h),int(0.05*h)), 'score: '+str(score)[:6], font=font, fill=(0,0,0))\n",
    "                    else:\n",
    "                        color=(0,0,0)\n",
    "                theta=state_history_list[j][i][0]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L*np.sin(theta)\n",
    "                y1=y0+L*np.cos(theta)\n",
    "                #---draw the image ----\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=color,width=1)\n",
    "                draw.ellipse([(x1-d,y1-d),(x1+d,y1+d)], fill=color, outline=None)\n",
    "        frames.append(img)\n",
    "    cv2_list=pil_list_to_cv2(frames)\n",
    "    generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "def render_double_pendulum(state_history_list,dt,best_idx=None,scores=None,actions=None,m1=1,m2=1,l1=1,l2=1,img_res=1.5,save_path='trash_figures/inverted_double_pendulum_generation.avi'):\n",
    "    frames_per_second=20\n",
    "    take_frame_every=int(1/(dt*frames_per_second))\n",
    "    frames=[]\n",
    "    h=int(img_res*200)\n",
    "    if actions is None:\n",
    "        w=h\n",
    "    else:\n",
    "        w=int(1.5*h)\n",
    "    x0=int(0.6*h)\n",
    "    y0=int(h/2)\n",
    "    h_red=int(0.4*h)\n",
    "    l_tot=l1+l2\n",
    "    l1_ratio=l1/l_tot\n",
    "    l2_ratio=l2/l_tot\n",
    "    L1=l1_ratio*h_red\n",
    "    L2=l2_ratio*h_red\n",
    "    d=int(0.02*h)\n",
    "    d1=d*m1**(1/3)\n",
    "    d2=d*m2**(1/3)\n",
    "    d_4=d/4\n",
    "    if best_idx is not None:\n",
    "        state_history_list.append(state_history_list[best_idx])\n",
    "    largest_trajectory=get_largest_length(state_history_list)\n",
    "    for i in range(largest_trajectory):\n",
    "        if i%take_frame_every==0:\n",
    "            img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            for j in range(len(state_history_list)): \n",
    "                if len(state_history_list[j])>i+1:\n",
    "                    if best_idx is not None:\n",
    "                        if j==len(state_history_list)-1:\n",
    "                            color1=(255,0,0)\n",
    "                            color2=(255,0,0)\n",
    "                            if actions is not None:\n",
    "                                F=actions[i]\n",
    "                                if F<0:\n",
    "                                    F_col=(100,100,255)\n",
    "                                else:\n",
    "                                    F_col=(255,100,100)\n",
    "                                d_F=0.2*np.abs(F)*d\n",
    "                                draw.ellipse([(x0-2*d_F,y0-2*d_F),(x0+2*d_F,y0+2*d_F)], fill=F_col, outline=(0,0,0))\n",
    "                                #draw.line([(u,v),(u,v-int(img_res*F))],fill=F_col,width=int(5*img_res))\n",
    "                                #font = ImageFont.truetype(\"arial.ttf\", int(h/18))\n",
    "                                #draw.text((int(u-0.3*h),v), 'force:', font=font, fill=(0,0,0))\n",
    "                            if scores is not None:\n",
    "                                score=int(scores[i])\n",
    "                                font = ImageFont.truetype(\"arial.ttf\", int(h/20))\n",
    "                                draw.text((int(0.1*h),int(0.05*h)), 'score: '+str(score)[:6], font=font, fill=(0,0,0))\n",
    "                        else:\n",
    "                            color1=(0,0,255)\n",
    "                            color2=(0,0,0)\n",
    "                    theta=state_history_list[j][i][0]\n",
    "                    theta1=state_history_list[j][i][0]\n",
    "                    theta2=state_history_list[j][i][1]\n",
    "                    #----transform to cartesian coordinates---\n",
    "                    x1=x0+L1*np.sin(theta1)\n",
    "                    y1=y0+L1*np.cos(theta1)\n",
    "                    x2=x1+L2*np.sin(theta2)\n",
    "                    y2=y1+L2*np.cos(theta2)\n",
    "                    #---draw the image ----\n",
    "                    draw.line([(x0,y0),(x1,y1)],fill=color2,width=1)\n",
    "                    draw.ellipse([(x1-d1,y1-d1),(x1+d1,y1+d1)], fill=color2, outline=None)\n",
    "                    draw.line([(x1,y1),(x2,y2)],fill=color2,width=1)\n",
    "                    draw.ellipse([(x2-d2,y2-d2),(x2+d2,y2+d2)], fill=color1, outline=None)\n",
    "        frames.append(img)\n",
    "    cv2_list=pil_list_to_cv2(frames)\n",
    "    generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "def pil_list_to_cv2(pil_list):\n",
    "    #converts a list of pil images to a list of cv2 images\n",
    "    png_list=[]\n",
    "    for pil_img in pil_list:\n",
    "        pil_img.save('trash_image.png',format='png')\n",
    "        png_list.append(cv2.imread('trash_image.png'))\n",
    "    os.remove('trash_image.png')\n",
    "    return png_list\n",
    "\n",
    "def generate_video(cv2_list,path='car_race.avi',fps=10): \n",
    "    #makes a video from a given cv2 image list\n",
    "    if len(cv2_list)==0:\n",
    "        raise ValueError('the given png list is empty!')\n",
    "    video_name = path\n",
    "    frame=cv2_list[0] \n",
    "    # setting the frame width, height width \n",
    "    # the width, height of first image \n",
    "    height, width, layers = frame.shape   \n",
    "    video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "    # Appending the images to the video one by one \n",
    "    for cv2_image in cv2_list:  \n",
    "        video.write(cv2_image) \n",
    "    # Deallocating memories taken for window creation \n",
    "    cv2.destroyAllWindows()  \n",
    "    video.release()  # releasing the video generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom up: False\n",
      "rendering iteration: 0/446\n",
      "generation: 1, best score: 7061.821, running average: 7061.821\n",
      "bottom up: False\n",
      "generation: 2, best score: 6123.825, running average: 7014.921\n",
      "bottom up: False\n",
      "generation: 3, best score: 6193.268, running average: 6973.839\n",
      "bottom up: False\n",
      "generation: 4, best score: 6305.886, running average: 6940.441\n",
      "bottom up: False\n",
      "generation: 5, best score: 4660.463, running average: 6826.442\n",
      "bottom up: False\n",
      "generation: 6, best score: 5011.316, running average: 6735.686\n",
      "bottom up: False\n",
      "generation: 7, best score: 4517.712, running average: 6624.787\n",
      "bottom up: False\n",
      "generation: 8, best score: 5697.449, running average: 6578.420\n",
      "bottom up: False\n",
      "generation: 9, best score: 4666.820, running average: 6482.840\n",
      "bottom up: False\n",
      "generation: 10, best score: 7797.739, running average: 6548.585\n",
      "bottom up: False\n",
      "generation: 11, best score: 3763.233, running average: 6409.318\n",
      "bottom up: False\n",
      "generation: 12, best score: 3799.920, running average: 6278.848\n",
      "bottom up: False\n",
      "generation: 13, best score: 3671.830, running average: 6148.497\n",
      "bottom up: False\n",
      "generation: 14, best score: 3531.900, running average: 6017.667\n",
      "bottom up: False\n",
      "generation: 15, best score: 3670.979, running average: 5900.333\n",
      "bottom up: False\n",
      "generation: 16, best score: 6202.591, running average: 5915.446\n",
      "bottom up: False\n",
      "generation: 17, best score: 4961.439, running average: 5867.745\n",
      "bottom up: False\n",
      "generation: 18, best score: 5483.199, running average: 5848.518\n",
      "bottom up: False\n",
      "generation: 19, best score: 3450.414, running average: 5728.613\n",
      "bottom up: False\n",
      "generation: 20, best score: 3726.382, running average: 5628.501\n",
      "bottom up: False\n",
      "generation: 21, best score: 3558.431, running average: 5524.998\n",
      "bottom up: False\n",
      "generation: 22, best score: 2258.279, running average: 5361.662\n",
      "bottom up: False\n",
      "generation: 23, best score: 3361.261, running average: 5261.642\n",
      "bottom up: False\n",
      "generation: 24, best score: 3978.458, running average: 5197.483\n",
      "bottom up: False\n",
      "generation: 25, best score: 5401.015, running average: 5207.659\n",
      "bottom up: False\n",
      "generation: 26, best score: 3683.621, running average: 5131.457\n",
      "bottom up: False\n",
      "generation: 27, best score: 4339.316, running average: 5091.850\n",
      "bottom up: False\n",
      "generation: 28, best score: 3559.122, running average: 5015.214\n",
      "bottom up: False\n",
      "generation: 29, best score: 6130.548, running average: 5070.981\n",
      "bottom up: False\n",
      "generation: 30, best score: 4061.078, running average: 5020.485\n",
      "bottom up: False\n",
      "generation: 31, best score: 4800.482, running average: 5009.485\n",
      "bottom up: False\n",
      "generation: 32, best score: 3067.904, running average: 4912.406\n",
      "bottom up: False\n",
      "generation: 33, best score: 4546.241, running average: 4894.098\n",
      "bottom up: False\n",
      "generation: 34, best score: 3401.523, running average: 4819.469\n",
      "bottom up: False\n",
      "generation: 35, best score: 2792.185, running average: 4718.105\n",
      "bottom up: False\n",
      "generation: 36, best score: 2276.430, running average: 4596.021\n",
      "bottom up: False\n",
      "generation: 37, best score: 3477.035, running average: 4540.072\n",
      "bottom up: False\n",
      "generation: 38, best score: 3691.790, running average: 4497.658\n",
      "bottom up: False\n",
      "generation: 39, best score: 4074.046, running average: 4476.477\n",
      "bottom up: False\n",
      "generation: 40, best score: 4249.464, running average: 4465.127\n",
      "bottom up: False\n",
      "generation: 41, best score: 4054.665, running average: 4444.603\n",
      "bottom up: False\n",
      "generation: 42, best score: 4117.934, running average: 4428.270\n",
      "bottom up: False\n",
      "generation: 43, best score: 4031.922, running average: 4408.453\n",
      "bottom up: False\n",
      "generation: 44, best score: 4437.781, running average: 4409.919\n",
      "bottom up: False\n",
      "generation: 45, best score: 3140.225, running average: 4346.434\n",
      "bottom up: False\n",
      "generation: 46, best score: 6725.130, running average: 4465.369\n",
      "bottom up: False\n",
      "generation: 47, best score: 3304.712, running average: 4407.336\n",
      "bottom up: False\n",
      "generation: 48, best score: 2982.349, running average: 4336.087\n",
      "bottom up: False\n",
      "generation: 49, best score: 3596.692, running average: 4299.117\n",
      "bottom up: False\n",
      "generation: 50, best score: 3722.520, running average: 4270.287\n",
      "bottom up: False\n",
      "rendering iteration: 0/258\n",
      "generation: 51, best score: 4117.190, running average: 4262.632\n",
      "bottom up: False\n",
      "generation: 52, best score: 3745.835, running average: 4236.793\n",
      "bottom up: False\n",
      "generation: 53, best score: 4759.904, running average: 4262.948\n",
      "bottom up: False\n",
      "generation: 54, best score: 6007.982, running average: 4350.200\n",
      "bottom up: False\n",
      "generation: 55, best score: 4270.184, running average: 4346.199\n",
      "bottom up: False\n",
      "generation: 56, best score: 4499.423, running average: 4353.860\n",
      "bottom up: False\n",
      "generation: 57, best score: 5053.900, running average: 4388.862\n",
      "bottom up: False\n",
      "generation: 58, best score: 4555.609, running average: 4397.200\n",
      "bottom up: False\n",
      "generation: 59, best score: 6023.682, running average: 4478.524\n",
      "bottom up: False\n",
      "generation: 60, best score: 3859.464, running average: 4447.571\n",
      "bottom up: False\n",
      "generation: 61, best score: 3469.770, running average: 4398.681\n",
      "bottom up: False\n",
      "generation: 62, best score: 4509.272, running average: 4404.210\n",
      "bottom up: False\n",
      "generation: 63, best score: 3468.981, running average: 4357.449\n",
      "bottom up: False\n",
      "generation: 64, best score: 2682.159, running average: 4273.684\n",
      "bottom up: False\n",
      "generation: 65, best score: 4913.761, running average: 4305.688\n",
      "bottom up: False\n",
      "generation: 66, best score: 3786.412, running average: 4279.724\n",
      "bottom up: False\n",
      "generation: 67, best score: 4198.859, running average: 4275.681\n",
      "bottom up: False\n",
      "generation: 68, best score: 4991.479, running average: 4311.471\n",
      "bottom up: False\n",
      "generation: 69, best score: 3412.128, running average: 4266.504\n",
      "bottom up: False\n",
      "generation: 70, best score: 2999.275, running average: 4203.142\n",
      "bottom up: False\n",
      "generation: 71, best score: 4796.768, running average: 4232.824\n",
      "bottom up: False\n",
      "generation: 72, best score: 3125.132, running average: 4177.439\n",
      "bottom up: False\n",
      "generation: 73, best score: 3529.506, running average: 4145.042\n",
      "bottom up: False\n",
      "generation: 74, best score: 4473.281, running average: 4161.454\n",
      "bottom up: False\n",
      "generation: 75, best score: 5010.434, running average: 4203.903\n",
      "bottom up: False\n",
      "generation: 76, best score: 2707.825, running average: 4129.099\n",
      "bottom up: False\n",
      "generation: 77, best score: 4426.057, running average: 4143.947\n",
      "bottom up: False\n",
      "generation: 78, best score: 6618.523, running average: 4267.676\n",
      "bottom up: False\n",
      "generation: 79, best score: 4067.688, running average: 4257.677\n",
      "bottom up: False\n",
      "generation: 80, best score: 3016.927, running average: 4195.639\n",
      "bottom up: False\n",
      "generation: 81, best score: 2549.031, running average: 4113.309\n",
      "bottom up: False\n",
      "generation: 82, best score: 4670.415, running average: 4141.164\n",
      "bottom up: False\n",
      "generation: 83, best score: 4630.638, running average: 4165.638\n",
      "bottom up: False\n",
      "generation: 84, best score: 2474.176, running average: 4081.065\n",
      "bottom up: False\n",
      "generation: 85, best score: 5341.797, running average: 4144.101\n",
      "bottom up: False\n",
      "generation: 86, best score: 5869.143, running average: 4230.353\n",
      "bottom up: False\n",
      "generation: 87, best score: 2382.264, running average: 4137.949\n",
      "bottom up: False\n",
      "generation: 88, best score: 4680.988, running average: 4165.101\n",
      "bottom up: False\n",
      "generation: 89, best score: 5692.302, running average: 4241.461\n",
      "bottom up: False\n",
      "generation: 90, best score: 4798.495, running average: 4269.313\n",
      "bottom up: False\n",
      "generation: 91, best score: 3384.467, running average: 4225.070\n",
      "bottom up: False\n",
      "generation: 92, best score: 4206.442, running average: 4224.139\n",
      "bottom up: False\n",
      "generation: 93, best score: 4320.807, running average: 4228.972\n",
      "bottom up: False\n",
      "generation: 94, best score: 3071.377, running average: 4171.093\n",
      "bottom up: False\n",
      "generation: 95, best score: 2999.397, running average: 4112.508\n",
      "bottom up: False\n",
      "generation: 96, best score: 4881.113, running average: 4150.938\n",
      "bottom up: False\n",
      "generation: 97, best score: 4690.237, running average: 4177.903\n",
      "bottom up: False\n",
      "generation: 98, best score: 3492.927, running average: 4143.654\n",
      "bottom up: False\n",
      "generation: 99, best score: 3064.741, running average: 4089.709\n",
      "bottom up: False\n",
      "generation: 100, best score: 3406.759, running average: 4055.561\n",
      "bottom up: False\n",
      "rendering iteration: 0/274\n",
      "generation: 101, best score: 3477.402, running average: 4026.653\n",
      "bottom up: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation: 102, best score: 4196.128, running average: 4035.127\n",
      "bottom up: False\n",
      "generation: 103, best score: 3414.392, running average: 4004.090\n",
      "bottom up: False\n",
      "generation: 104, best score: 6554.721, running average: 4131.622\n",
      "bottom up: False\n",
      "generation: 105, best score: 2622.153, running average: 4056.148\n",
      "bottom up: False\n",
      "generation: 106, best score: 3515.595, running average: 4029.121\n",
      "bottom up: False\n",
      "generation: 107, best score: 3473.310, running average: 4001.330\n",
      "bottom up: False\n",
      "generation: 108, best score: 4674.043, running average: 4034.966\n",
      "bottom up: False\n",
      "generation: 109, best score: 3675.481, running average: 4016.991\n",
      "bottom up: False\n",
      "generation: 110, best score: 4596.416, running average: 4045.963\n",
      "bottom up: False\n",
      "generation: 111, best score: 4193.413, running average: 4053.335\n",
      "bottom up: False\n",
      "generation: 112, best score: 3506.025, running average: 4025.970\n",
      "bottom up: False\n",
      "generation: 113, best score: 5435.718, running average: 4096.457\n",
      "bottom up: False\n",
      "generation: 114, best score: 5350.860, running average: 4159.177\n",
      "bottom up: False\n",
      "generation: 115, best score: 2960.773, running average: 4099.257\n",
      "bottom up: False\n",
      "generation: 116, best score: 4720.193, running average: 4130.304\n",
      "bottom up: False\n",
      "generation: 117, best score: 4678.096, running average: 4157.693\n",
      "bottom up: False\n",
      "generation: 118, best score: 3043.562, running average: 4101.987\n",
      "bottom up: False\n",
      "generation: 119, best score: 4186.294, running average: 4106.202\n",
      "bottom up: False\n",
      "generation: 120, best score: 6967.638, running average: 4249.274\n",
      "bottom up: False\n",
      "generation: 121, best score: 4451.279, running average: 4259.374\n",
      "bottom up: False\n",
      "generation: 122, best score: 3969.690, running average: 4244.890\n",
      "bottom up: False\n",
      "generation: 123, best score: 3718.524, running average: 4218.572\n",
      "bottom up: False\n",
      "generation: 124, best score: 4324.472, running average: 4223.867\n",
      "bottom up: False\n",
      "generation: 125, best score: 7293.010, running average: 4377.324\n",
      "bottom up: False\n",
      "generation: 126, best score: 3193.386, running average: 4318.127\n",
      "bottom up: False\n",
      "generation: 127, best score: 5482.552, running average: 4376.348\n",
      "bottom up: False\n",
      "generation: 128, best score: 3203.100, running average: 4317.686\n",
      "bottom up: False\n",
      "generation: 129, best score: 3257.749, running average: 4264.689\n",
      "bottom up: False\n",
      "generation: 130, best score: 4418.937, running average: 4272.401\n",
      "bottom up: False\n",
      "generation: 131, best score: 4173.404, running average: 4267.452\n",
      "bottom up: False\n",
      "generation: 132, best score: 3792.164, running average: 4243.687\n",
      "bottom up: False\n",
      "generation: 133, best score: 4560.916, running average: 4259.549\n",
      "bottom up: False\n",
      "generation: 134, best score: 5621.091, running average: 4327.626\n",
      "bottom up: False\n",
      "generation: 135, best score: 4629.544, running average: 4342.722\n",
      "bottom up: False\n",
      "generation: 136, best score: 3461.234, running average: 4298.647\n",
      "bottom up: False\n",
      "generation: 137, best score: 4462.159, running average: 4306.823\n",
      "bottom up: False\n",
      "generation: 138, best score: 3580.568, running average: 4270.510\n",
      "bottom up: False\n",
      "generation: 139, best score: 4316.674, running average: 4272.818\n",
      "bottom up: False\n",
      "generation: 140, best score: 4043.653, running average: 4261.360\n",
      "bottom up: False\n",
      "generation: 141, best score: 4722.896, running average: 4284.437\n",
      "bottom up: False\n",
      "generation: 142, best score: 4885.199, running average: 4314.475\n",
      "bottom up: False\n",
      "generation: 143, best score: 3748.641, running average: 4286.183\n",
      "bottom up: False\n",
      "generation: 144, best score: 3114.677, running average: 4227.608\n",
      "bottom up: False\n",
      "generation: 145, best score: 6048.716, running average: 4318.663\n",
      "bottom up: False\n",
      "generation: 146, best score: 4667.348, running average: 4336.098\n",
      "bottom up: False\n",
      "generation: 147, best score: 4323.725, running average: 4335.479\n",
      "bottom up: False\n",
      "generation: 148, best score: 3577.017, running average: 4297.556\n",
      "bottom up: False\n",
      "generation: 149, best score: 4516.973, running average: 4308.527\n",
      "bottom up: False\n",
      "generation: 150, best score: 3572.109, running average: 4271.706\n",
      "bottom up: False\n",
      "rendering iteration: 0/231\n",
      "generation: 151, best score: 3609.240, running average: 4238.583\n",
      "bottom up: False\n",
      "generation: 152, best score: 4125.614, running average: 4232.934\n",
      "bottom up: False\n",
      "generation: 153, best score: 4128.396, running average: 4227.707\n",
      "bottom up: False\n",
      "generation: 154, best score: 3931.503, running average: 4212.897\n",
      "bottom up: False\n",
      "generation: 155, best score: 3493.045, running average: 4176.904\n",
      "bottom up: False\n",
      "generation: 156, best score: 3772.813, running average: 4156.700\n",
      "bottom up: False\n",
      "generation: 157, best score: 4246.026, running average: 4161.166\n",
      "bottom up: False\n",
      "generation: 158, best score: 6828.690, running average: 4294.542\n",
      "bottom up: False\n",
      "generation: 159, best score: 6757.845, running average: 4417.708\n",
      "bottom up: False\n",
      "generation: 160, best score: 5326.755, running average: 4463.160\n",
      "bottom up: False\n",
      "generation: 161, best score: 2646.503, running average: 4372.327\n",
      "bottom up: False\n",
      "generation: 162, best score: 6532.968, running average: 4480.359\n",
      "bottom up: False\n",
      "generation: 163, best score: 4904.134, running average: 4501.548\n",
      "bottom up: False\n",
      "generation: 164, best score: 3545.172, running average: 4453.729\n",
      "bottom up: False\n",
      "generation: 165, best score: 3974.953, running average: 4429.790\n",
      "bottom up: False\n",
      "generation: 166, best score: 3701.539, running average: 4393.378\n",
      "bottom up: False\n",
      "generation: 167, best score: 5546.154, running average: 4451.017\n",
      "bottom up: False\n",
      "generation: 168, best score: 4057.272, running average: 4431.329\n",
      "bottom up: False\n",
      "generation: 169, best score: 3297.364, running average: 4374.631\n",
      "bottom up: False\n",
      "generation: 170, best score: 3374.836, running average: 4324.641\n",
      "bottom up: False\n",
      "generation: 171, best score: 3421.373, running average: 4279.478\n",
      "bottom up: False\n",
      "generation: 172, best score: 3261.570, running average: 4228.582\n",
      "bottom up: False\n",
      "generation: 173, best score: 4725.267, running average: 4253.417\n",
      "bottom up: False\n",
      "generation: 174, best score: 2277.194, running average: 4154.606\n",
      "bottom up: False\n",
      "generation: 175, best score: 3372.488, running average: 4115.500\n",
      "bottom up: False\n",
      "generation: 176, best score: 2965.844, running average: 4058.017\n",
      "bottom up: False\n",
      "generation: 177, best score: 3706.904, running average: 4040.461\n",
      "bottom up: False\n",
      "generation: 178, best score: 2525.011, running average: 3964.689\n",
      "bottom up: False\n",
      "generation: 179, best score: 3190.534, running average: 3925.981\n",
      "bottom up: False\n",
      "generation: 180, best score: 3781.208, running average: 3918.742\n",
      "bottom up: False\n",
      "generation: 181, best score: 3607.418, running average: 3903.176\n",
      "bottom up: False\n",
      "generation: 182, best score: 5384.191, running average: 3977.227\n",
      "bottom up: False\n",
      "generation: 183, best score: 7483.731, running average: 4152.552\n",
      "bottom up: False\n",
      "generation: 184, best score: 4063.143, running average: 4148.082\n",
      "bottom up: False\n",
      "generation: 185, best score: 5110.466, running average: 4196.201\n",
      "bottom up: False\n",
      "generation: 186, best score: 2905.430, running average: 4131.662\n",
      "bottom up: False\n",
      "generation: 187, best score: 4960.605, running average: 4173.109\n",
      "bottom up: False\n",
      "generation: 188, best score: 3022.360, running average: 4115.572\n",
      "bottom up: False\n",
      "generation: 189, best score: 5683.096, running average: 4193.948\n",
      "bottom up: False\n",
      "generation: 190, best score: 4239.094, running average: 4196.206\n",
      "bottom up: False\n",
      "generation: 191, best score: 3921.745, running average: 4182.483\n",
      "bottom up: False\n",
      "generation: 192, best score: 6733.654, running average: 4310.041\n",
      "bottom up: False\n",
      "generation: 193, best score: 5364.083, running average: 4362.743\n",
      "bottom up: False\n",
      "generation: 194, best score: 5926.957, running average: 4440.954\n",
      "bottom up: False\n",
      "generation: 195, best score: 3435.794, running average: 4390.696\n",
      "bottom up: False\n",
      "generation: 196, best score: 4247.550, running average: 4383.539\n",
      "bottom up: False\n",
      "generation: 197, best score: 3365.547, running average: 4332.639\n",
      "bottom up: False\n",
      "generation: 198, best score: 3675.088, running average: 4299.761\n",
      "bottom up: False\n",
      "generation: 199, best score: 4495.138, running average: 4309.530\n",
      "bottom up: False\n",
      "generation: 200, best score: 3289.481, running average: 4258.528\n",
      "bottom up: False\n",
      "rendering iteration: 0/275\n",
      "generation: 201, best score: 4807.068, running average: 4285.955\n",
      "bottom up: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation: 202, best score: 4729.636, running average: 4308.139\n",
      "bottom up: False\n",
      "generation: 203, best score: 3430.971, running average: 4264.281\n",
      "bottom up: False\n",
      "generation: 204, best score: 5151.367, running average: 4308.635\n",
      "bottom up: False\n",
      "generation: 205, best score: 4171.450, running average: 4301.776\n",
      "bottom up: False\n",
      "generation: 206, best score: 4675.342, running average: 4320.454\n",
      "bottom up: False\n",
      "generation: 207, best score: 4331.769, running average: 4321.020\n",
      "bottom up: False\n",
      "generation: 208, best score: 3127.421, running average: 4261.340\n",
      "bottom up: False\n",
      "generation: 209, best score: 5030.180, running average: 4299.782\n",
      "bottom up: False\n"
     ]
    }
   ],
   "source": [
    "#simulation settings\n",
    "dt=0.03\n",
    "\n",
    "#evolutionary parameters\n",
    "n_generations=301\n",
    "population_size=30\n",
    "n_survivors=10\n",
    "n_neurons=[4,100,1]\n",
    "mutation_rate=0.01\n",
    "\n",
    "#further regularization settings\n",
    "add_noise=0\n",
    "disturb=True\n",
    "max_steps=500\n",
    "bottom_up=False\n",
    "alternate=False\n",
    "\n",
    "#further settings\n",
    "use_symmetry=True\n",
    "load_best_agent=True\n",
    "make_video=True\n",
    "\n",
    "agent=GeneticAgent(n_neurons,bottom_up=bottom_up,use_symmetry=use_symmetry)\n",
    "new_agents=get_first_generation(population_size,agent,n_neurons,mutation_rate=mutation_rate,load_best_agent=load_best_agent)#all agents initialized independently at random\n",
    "new_agents[0].render_action_space('inverted_double_pendulum/action_space_init.jpeg',img_res=2)\n",
    "running_score=0\n",
    "for i in range(n_generations):\n",
    "    state_history_list=[]\n",
    "    if alternate:\n",
    "        bottom_up=0.5>np.random.rand()\n",
    "        if bottom_up:\n",
    "            add_noise=0.3\n",
    "        else:\n",
    "            add_noise=0.6\n",
    "    print('bottom up: '+str(bottom_up))\n",
    "    if n_neurons[0]==2:\n",
    "        env=EnvironmentInvertedPendulum(max_steps=max_steps,dt=dt,m=5,add_noise=add_noise,bottom_up=bottom_up,disturb=disturb)\n",
    "    else:\n",
    "        env=EnvironmentInvertedDoublePendulum(max_steps=max_steps,dt=dt,m1=0.5,m2=1,l1=1,l2=1,add_noise=add_noise,bottom_up=bottom_up,disturb=disturb)\n",
    "    scores,environments=get_scores(new_agents,env)\n",
    "    old_agents=copy.deepcopy(new_agents)\n",
    "    new_agents,rank_idx=get_next_generation(scores,new_agents,n_survivors,population_size,mutation_rate,store_best_agent=True)\n",
    "    best_actions=old_agents[rank_idx[-1]].actions\n",
    "    best_scores=old_agents[rank_idx[-1]].scores\n",
    "    if disturb:\n",
    "        disturb_history=environments[rank_idx[-1]].disturb_history\n",
    "    else:\n",
    "        disturb_history=None\n",
    "    best_score=scores[rank_idx[-1]]\n",
    "    for environment in environments:\n",
    "        state_history_list.append(environment.state_history)\n",
    "    if i%50==0:\n",
    "        environments[rank_idx[-1]].render(img_res=2.5,scores=best_scores,disturb_history=disturb_history,save_path='inverted_double_pendulum/single_inverted_doublependulum_g='+str(i)+'_bu='+str(bottom_up)+'.avi')\n",
    "        #old_agents[rank_idx[-1]].render_action_space('inverted_double_pendulum/action_space_'+str(i)+'.jpeg',img_res=3,trajectory_list=state_history_list,make_video=True)\n",
    "    if make_video and i%50==0:\n",
    "        if n_neurons[0]==2:\n",
    "            render_single_pendulum(state_history_list,best_idx=rank_idx[-1],scores=best_scores,actions=best_actions,dt=0.03,m=1,img_res=2.5,disturb_history=disturb_history,save_path='inverted_pendulum/inverted_pendulum_gen='+str(i)+'_bu='+str(bottom_up)+'.avi')\n",
    "        else:\n",
    "            render_double_pendulum(state_history_list,best_idx=rank_idx[-1],scores=best_scores,actions=best_actions,dt=0.03,m1=0.5,m2=1,l1=1,l2=1,img_res=2.5,save_path='inverted_double_pendulum/inverted_double_pendulum_gen='+str(i)+'_bu='+str(bottom_up)+'.avi')\n",
    "    if i==0:\n",
    "        running_score+=best_score\n",
    "    else:\n",
    "        running_score=0.95*running_score+0.05*best_score\n",
    "    \n",
    "    print('generation: '+str(i+1)+', best score: '+str(best_score)[:8]+', running average: '+str(running_score)[:8])\n",
    "#---render epoch--\n",
    "print('render environment...')\n",
    "environments[rank_idx[-1]].render(img_res=2.5,save_path='inverted_double_pendulum/single_inverted_doublependulum.avi')\n",
    "old_agents[rank_idx[-1]].render_action_space('inverted_double_pendulum/action_space_end.jpeg',img_res=3)\n",
    "\n",
    "\n",
    "#---check the energy\n",
    "#e_pot_start,e_kin_start=env.get_energy(env.state_history[0])\n",
    "#e_pot_end,e_kin_end=env.get_energy(env.state_history[-1])\n",
    "#e_start=np.sum(e_pot_start)+np.sum(e_kin_start)\n",
    "#e_end=np.sum(e_pot_end)+np.sum(e_kin_end)\n",
    "#print('starting energy: '+str(e_start))\n",
    "#print('final energy: '+str(e_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
