{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1396a8490>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stuff we need\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import copy \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from routines import *\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    #initialization of internal state\n",
    "    def __init__(self,m1=1,m2=1,l1=1,l2=1,g=1,max_steps=200,dt=0.005):\n",
    "        self.m1=m1\n",
    "        self.m2=m2\n",
    "        self.l1=l1\n",
    "        self.l2=l2\n",
    "        self.g=g\n",
    "        self.dt=dt\n",
    "        self.steps_left=max_steps\n",
    "        self.reset()\n",
    "\n",
    "        \n",
    "    def reset(self,theta1=np.pi+0.03*np.random.rand(),theta2=np.pi+0.05*np.random.rand(),theta1_d=0,theta2_d=0):\n",
    "        self.state=[theta1,theta2,theta1_d,theta2_d]\n",
    "        self.state_history=[self.state.copy()]\n",
    "        \n",
    "    #returns the current environment's observation to the agent\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    #allows the agent to query the set of actions it can execute\n",
    "    def sample_action(self):\n",
    "        #return self.state[3]+0.1*(np.random.rand()-0.5)\n",
    "        return 1.2\n",
    "\n",
    "    \n",
    "    #signals the end of the episode to the agent\n",
    "    def is_done(self):\n",
    "        return self.steps_left==0\n",
    "    \n",
    "    #central piece: handles agents action and returns reward for the action\n",
    "    def action(self,action):\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is over')\n",
    "        self.step(action)\n",
    "        delta=np.abs(self.state[1]-np.pi)\n",
    "        if delta<np.pi/2:\n",
    "            self.steps_left -=1\n",
    "            return 1-delta/(np.pi/2)\n",
    "        else:\n",
    "            self.steps_left=0\n",
    "            #self.steps_left -=1#if you want to play the game until max step reached\n",
    "            return 0\n",
    "    \n",
    "    def explicite_euler(self,dt,state,F=0):\n",
    "        theta1_dd,theta2_dd=self.get_theta_dd(state,F)\n",
    "        next_state= [state[0]+dt*state[2],state[1]+dt*state[3],state[2]+dt*theta1_dd,state[3]+dt*theta2_dd]\n",
    "        return next_state\n",
    "    \n",
    "    #the decoupled equations of motion\n",
    "    def get_theta_dd(self,state,F=0):\n",
    "        theta1=state[0]\n",
    "        theta2=state[1]\n",
    "        theta1_d=state[2]\n",
    "        theta2_d=state[3]\n",
    "        #----theta1_dd-----\n",
    "        num1=-self.g*((2*self.m1+self.m2)*np.sin(theta1)+self.m2*np.sin(theta1-2*theta2))\n",
    "        num2=-2*np.sin(theta1-theta2)*self.m2*(theta2_d**2*self.l2+theta1_d**2*self.l1*np.cos(theta1-theta2))\n",
    "        num3=2*F\n",
    "        denum1=2*self.m1+self.m2-self.m2*np.cos(2*theta1-2*theta2)\n",
    "        denum=self.l1*denum1\n",
    "        theta1_dd=(num1+num2+num3)/denum\n",
    "        #----theta2_dd----\n",
    "        num1=2*np.sin(theta1-theta2)\n",
    "        num2=theta1_d**2*self.l1*(self.m1+self.m2)+self.g*(self.m1+self.m2)*np.cos(theta1)+theta2_d**2*self.l2*self.m2*np.cos(theta1-theta2)\n",
    "        num3=-2*F*np.cos(theta1-theta2)\n",
    "        denum=self.l2*denum1\n",
    "        theta2_dd=(num3+num1*num2)/denum\n",
    "        return theta1_dd,theta2_dd\n",
    "    \n",
    "    #differentail time step using explicite midpoint method\n",
    "    def step(self,F):\n",
    "        next_state=self.explicite_euler(self.dt/2,self.state,F)\n",
    "        theta1_dd,theta2_dd=self.get_theta_dd(next_state,F)\n",
    "        self.state[0]+=self.dt*next_state[2]\n",
    "        self.state[1]+=self.dt*next_state[3]\n",
    "        self.state[2]+=self.dt*theta1_dd\n",
    "        self.state[3]+=self.dt*theta2_dd\n",
    "        self.state_history.append(self.state.copy())\n",
    "        \n",
    "    \n",
    "    def render(self,img_res=1,save_path='trash_figures/'):\n",
    "        frames_per_second=20\n",
    "        take_frame_every=int(1/(self.dt*frames_per_second))\n",
    "        frames=[]\n",
    "        h=int(img_res*200)\n",
    "        w=h\n",
    "        x0=int(w/2)\n",
    "        y0=int(h/2)\n",
    "        h_red=int(0.4*h)\n",
    "        l_tot=self.l1+self.l2\n",
    "        l1_ratio=self.l1/l_tot\n",
    "        l2_ratio=self.l2/l_tot\n",
    "        L1=l1_ratio*h_red\n",
    "        L2=l2_ratio*h_red\n",
    "        d=int(0.02*h)\n",
    "        d1=d*self.m1**(1/3)\n",
    "        d2=d*self.m2**(1/3)\n",
    "        d_4=d/4\n",
    "        #max_theta2_d=1.2*np.max(np.abs(phase_traject[:,3]))\n",
    "        for i,state_i in enumerate(self.state_history):\n",
    "            if i%5000==0:\n",
    "                print('rendering iteration: '+str(i)+'/'+str(len(self.state_history)))           \n",
    "            if i%take_frame_every==0:\n",
    "                theta1=state_i[0]\n",
    "                theta2=state_i[1]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L1*np.sin(theta1)\n",
    "                y1=y0+L1*np.cos(theta1)\n",
    "                x2=x1+L2*np.sin(theta2)\n",
    "                y2=y1+L2*np.cos(theta2)\n",
    "                #---draw the image ----\n",
    "                img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x1-d1,y1-d1),(x1+d1,y1+d1)], fill=(0,0,0), outline=None)\n",
    "                draw.line([(x1,y1),(x2,y2)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x2-d2,y2-d2),(x2+d2,y2+d2)], fill=(0,0,255), outline=None)\n",
    "                frames.append(img)\n",
    "        cv2_list=self.pil_list_to_cv2(frames)\n",
    "        self.generate_video(cv2_list,path=save_path+'inverted_pendulum.avi',fps=1000/40)\n",
    "    \n",
    "    #calculates the potential and kinetic energy of the two masses at a given state\n",
    "    def get_energy(self,state):\n",
    "        theta1=state[0]\n",
    "        theta2=state[1]\n",
    "        theta1_d=state[2]\n",
    "        theta2_d=state[3]\n",
    "        y1=self.l1*np.cos(theta1)\n",
    "        y2=y1+self.l2*np.cos(theta2)\n",
    "        e_pot=np.array([-self.m1*self.g*y1,-self.m2*self.g*y2])\n",
    "        e_kin_1=self.m1/2*(self.l1*theta1_d)**2\n",
    "        e_kin_2=(self.l1*theta1_d)**2\n",
    "        e_kin_2+=(self.l2*theta2_d)**2\n",
    "        e_kin_2+=2*self.l1*self.l2*theta1_d*theta2_d*(np.cos(theta1)*np.cos(theta2)+np.sin(theta1)*np.sin(theta2))\n",
    "        e_kin_2*=self.m2/2\n",
    "        e_kin=np.array([e_kin_1,e_kin_2])\n",
    "        return e_pot,e_kin\n",
    "    \n",
    "    #used for video converting\n",
    "    def pil_list_to_cv2(self,pil_list):\n",
    "        #converts a list of pil images to a list of cv2 images\n",
    "        png_list=[]\n",
    "        for pil_img in pil_list:\n",
    "            pil_img.save('trash_image.png',format='png')\n",
    "            png_list.append(cv2.imread('trash_image.png'))\n",
    "        os.remove('trash_image.png')\n",
    "        return png_list\n",
    "\n",
    "    def generate_video(self,cv2_list,path='car_race.avi',fps=10): \n",
    "        #makes a video from a given cv2 image list\n",
    "        if len(cv2_list)==0:\n",
    "            raise ValueError('the given png list is empty!')\n",
    "        video_name = path\n",
    "        frame=cv2_list[0] \n",
    "        # setting the frame width, height width \n",
    "        # the width, height of first image \n",
    "        height, width, layers = frame.shape   \n",
    "        video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "        # Appending the images to the video one by one \n",
    "        for cv2_image in cv2_list:  \n",
    "            video.write(cv2_image) \n",
    "        # Deallocating memories taken for window creation \n",
    "        cv2.destroyAllWindows()  \n",
    "        video.release()  # releasing the video generated \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAgent:\n",
    "    #initialize the counter for the total reward\n",
    "    def __init__(self,n_neurons=[3,2]):\n",
    "        self.total_reward=0.0\n",
    "        self.initialize_policy(n_neurons)\n",
    "            \n",
    "    #accepts the environment instance as an argument and allows the agents to observe and act\n",
    "    def step(self,env):\n",
    "        observation=env.get_observation()\n",
    "        #action = env.sample_action()\n",
    "        action=self.get_action(observation)\n",
    "        reward=env.action(action)\n",
    "        self.total_reward+=reward\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        with torch.no_grad():\n",
    "            return self.policy(torch.FloatTensor(state)).item()\n",
    "        \n",
    "    def mutate(self,muatation_rate=0.1):\n",
    "        with torch.no_grad():\n",
    "            for param in self.policy.parameters():\n",
    "                param.add_(torch.randn(param.size()) * muatation_rate)\n",
    "        \n",
    "    def initialize_policy(self,n_neurons):\n",
    "        self.policy=self.get_nn(4,1,n_neurons)\n",
    "    \n",
    "    def get_nn(self,input_dim,output_dim,n_neurons):\n",
    "        neural_network=nn.Sequential()\n",
    "        depth=len(n_neurons)\n",
    "        expanded_n_neurons=[input_dim]\n",
    "        expanded_n_neurons+=n_neurons\n",
    "        expanded_n_neurons.append(output_dim)\n",
    "        for i in range(depth):\n",
    "            neural_network.add_module(\"layer\"+str(i),nn.Sequential(nn.Linear(expanded_n_neurons[i],expanded_n_neurons[i+1]),nn.Sigmoid()))\n",
    "        neural_network.add_module(\"layer\"+str(depth),nn.Sequential(nn.Linear(expanded_n_neurons[depth],expanded_n_neurons[depth+1])))\n",
    "        return neural_network\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for genetic algorithm\n",
    "def get_first_generations(population_size):\n",
    "    pass\n",
    "\n",
    "def get_scores(agent_list,environment):\n",
    "    pass\n",
    "\n",
    "def get_next_generation(scores,mutants,n_survivors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 907.7033\n",
      "Total reward got: 531.6779\n",
      "Total reward got: 413.6388\n",
      "Total reward got: 577.3973\n",
      "Total reward got: 528.6088\n",
      "Total reward got: 1015.5056\n",
      "Total reward got: 597.1271\n",
      "Total reward got: 1279.2166\n",
      "Total reward got: 609.4569\n",
      "Total reward got: 389.6676\n",
      "rendering iteration: 0/534\n"
     ]
    }
   ],
   "source": [
    "n_generations=3\n",
    "n_mutants=10\n",
    "n_survivors=2\n",
    "\n",
    "new_mutants=get_first_generation(N_mutants)#all agents initialized independently at random\n",
    "for i in range(n_generations):\n",
    "    env=Environment(max_steps=3000,dt=0.005,m1=0.5,m2=1,l1=0.5,l2=1)\n",
    "    #env.reset(theta1=0,theta2=0)\n",
    "    scores=get_scores(new_mutants,env)\n",
    "    new_mutants,sorted_scores=get_next_generation(scores,mutants,n_survivors)\n",
    "    print('best score: '+str(sorted_scores[-1])[:6])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mutants=[]\n",
    "for i in range(N_mutants):\n",
    "    mutants.append(GeneticAgent())\n",
    "\n",
    "\n",
    "for mutant_agent in mutants:\n",
    "    env=Environment(max_steps=3000,dt=0.005,m1=0.5,m2=1,l1=0.5,l2=1)\n",
    "    #env.reset(theta1=0,theta2=0)\n",
    "    while not env.is_done():\n",
    "        mutant_agent.step(env)\n",
    "    print('Total reward got: %.4f' %mutant_agent.total_reward)\n",
    "\n",
    "#for i in range(N_mutants):\n",
    "    #mutant_agent=copy.deepcopy(agent)\n",
    "    #mutant_agent.mutate()\n",
    "    #mutants.append(mutant_agent)\n",
    "#---render epoch--\n",
    "env.render()\n",
    "\n",
    "#---check the energy\n",
    "#e_pot_start,e_kin_start=env.get_energy(env.state_history[0])\n",
    "#e_pot_end,e_kin_end=env.get_energy(env.state_history[-1])\n",
    "#e_start=np.sum(e_pot_start)+np.sum(e_kin_start)\n",
    "#e_end=np.sum(e_pot_end)+np.sum(e_kin_end)\n",
    "#print('starting energy: '+str(e_start))\n",
    "#print('final energy: '+str(e_end))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
