{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12c233830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stuff we need\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import copy \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import math\n",
    "from routines import *\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentInvertedPendulum:\n",
    "    #initialization of internal state\n",
    "    def __init__(self,m=1,l=1,g=1,max_steps=200,dt=0.005,add_noise=0.01,bottom_up=False):\n",
    "        self.m=m\n",
    "        self.l=l\n",
    "        self.g=g\n",
    "        self.dt=dt\n",
    "        self.steps_left=max_steps\n",
    "        self.reset(add_noise=add_noise)\n",
    "        self.bottom_up=bottom_up\n",
    "        if bottom_up:\n",
    "            self.reset(theta=0+add_noise*np.random.rand())\n",
    "            self.passed=False\n",
    "        else:\n",
    "            self.reset(add_noise=add_noise)\n",
    "\n",
    "    def reset(self,theta=np.pi,theta_d=0,add_noise=0):\n",
    "        self.state=[theta+add_noise*np.random.rand(),theta_d+add_noise*np.random.rand()]\n",
    "        self.state_history=[self.state.copy()]\n",
    "        \n",
    "    #returns the current environment's observation to the agent\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    #allows the agent to query the set of actions it can execute\n",
    "    def sample_action(self):\n",
    "        #return self.state[3]+0.1*(np.random.rand()-0.5)\n",
    "        return 1.2\n",
    "    \n",
    "    def get_energy(self,state):\n",
    "        e_kin=self.m/2*(self.l*state[1])**2\n",
    "        e_pot=-self.m*self.g*self.l*np.cos(state[0])\n",
    "        return e_kin,e_pot\n",
    "    \n",
    "    #signals the end of the episode to the agent\n",
    "    def is_done(self):\n",
    "        return self.steps_left==0\n",
    "    \n",
    "    #central piece: handles agents action and returns reward for the action\n",
    "    def action(self,action):\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is over')\n",
    "        self.step(action)\n",
    "        delta=np.abs(self.state[0]-np.pi)\n",
    "        if self.bottom_up:\n",
    "            e_pot,e_kin=self.get_energy(self.state)\n",
    "            e_tot=e_pot+e_kin\n",
    "            if e_tot>10:\n",
    "                #energy constraint:\n",
    "                self.steps_left=0\n",
    "                return 0\n",
    "            if delta<1:\n",
    "                self.passed=True\n",
    "            if self.passed:\n",
    "                if delta<np.pi/2:\n",
    "                    self.steps_left -=1\n",
    "                    return 10-delta**2\n",
    "                else:\n",
    "                    self.steps_left=0\n",
    "                    return 0\n",
    "            else:\n",
    "                self.steps_left -=1\n",
    "                return 3-delta**2\n",
    "        else:\n",
    "            if delta<np.pi/4:\n",
    "                self.steps_left -=1\n",
    "                return 2-delta**2\n",
    "            else:\n",
    "                self.steps_left=0\n",
    "                return 0\n",
    "    \n",
    "    def explicite_euler(self,dt,state,F=0):\n",
    "        theta_dd=self.get_theta_dd(state,F)\n",
    "        next_state= [state[0]+dt*state[1],state[1]+dt*theta_dd]\n",
    "        return next_state\n",
    "    \n",
    "    #the decoupled equations of motion\n",
    "    def get_theta_dd(self,state,F=0):\n",
    "        return F/(self.m*self.l)-self.g*np.sin(state[0])/self.l\n",
    "\n",
    "    \n",
    "    #differentail time step using explicite midpoint method\n",
    "    def step(self,F):\n",
    "        next_state=self.explicite_euler(self.dt/2,self.state,F)\n",
    "        theta_dd=self.get_theta_dd(next_state,F)\n",
    "        self.state[0]+=self.dt*next_state[1]\n",
    "        self.state[1]+=self.dt*theta_dd\n",
    "        self.state_history.append(self.state.copy())\n",
    "        \n",
    "    def render(self,img_res=1,save_path='trash_figures/inverted_pendulum.avi'):\n",
    "        frames_per_second=20\n",
    "        take_frame_every=int(1/(self.dt*frames_per_second))\n",
    "        frames=[]\n",
    "        h=int(img_res*200)\n",
    "        w=h\n",
    "        x0=int(w/2)\n",
    "        y0=int(h/2)\n",
    "        h_red=int(0.4*h)\n",
    "        L=h_red\n",
    "        d=int(0.02*h)\n",
    "        d=d*self.m**(1/3)\n",
    "        #max_theta2_d=1.2*np.max(np.abs(phase_traject[:,3]))\n",
    "        for i,state_i in enumerate(self.state_history):\n",
    "            if i%5000==0:\n",
    "                print('rendering iteration: '+str(i)+'/'+str(len(self.state_history)))           \n",
    "            if i%take_frame_every==0:\n",
    "                theta=state_i[0]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L*np.sin(theta)\n",
    "                y1=y0+L*np.cos(theta)\n",
    "                #---draw the image ----\n",
    "                img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x1-d,y1-d),(x1+d,y1+d)], fill=(0,0,0), outline=None)\n",
    "                frames.append(img)\n",
    "        cv2_list=self.pil_list_to_cv2(frames)\n",
    "        self.generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "    #calculates the potential and kinetic energy of the two masses at a given state\n",
    "    def get_energy(self,state):\n",
    "        theta=state[0]\n",
    "        theta_d=state[1]\n",
    "        y=self.l*np.cos(theta)\n",
    "        e_pot=-self.m*self.g*y\n",
    "        e_kin=self.m/2*(self.l*theta_d)**2\n",
    "        return e_pot,e_kin\n",
    "    \n",
    "    #used for video converting\n",
    "    def pil_list_to_cv2(self,pil_list):\n",
    "        #converts a list of pil images to a list of cv2 images\n",
    "        png_list=[]\n",
    "        for pil_img in pil_list:\n",
    "            pil_img.save('trash_image.png',format='png')\n",
    "            png_list.append(cv2.imread('trash_image.png'))\n",
    "        os.remove('trash_image.png')\n",
    "        return png_list\n",
    "\n",
    "    def generate_video(self,cv2_list,path='car_race.avi',fps=10): \n",
    "        #makes a video from a given cv2 image list\n",
    "        if len(cv2_list)==0:\n",
    "            raise ValueError('the given png list is empty!')\n",
    "        video_name = path\n",
    "        frame=cv2_list[0] \n",
    "        # setting the frame width, height width \n",
    "        # the width, height of first image \n",
    "        height, width, layers = frame.shape   \n",
    "        video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "        # Appending the images to the video one by one \n",
    "        for cv2_image in cv2_list:  \n",
    "            video.write(cv2_image) \n",
    "        # Deallocating memories taken for window creation \n",
    "        cv2.destroyAllWindows()  \n",
    "        video.release()  # releasing the video generated \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentInvertedDoublePendulum:\n",
    "    #initialization of internal state\n",
    "    def __init__(self,m1=1,m2=1,l1=1,l2=1,g=1,max_steps=200,dt=0.005,add_noise=0.01,bottom_up=False):\n",
    "        self.m1=m1\n",
    "        self.m2=m2\n",
    "        self.l1=l1\n",
    "        self.l2=l2\n",
    "        self.g=g\n",
    "        self.dt=dt\n",
    "        self.steps_left=max_steps\n",
    "        self.bottom_up=bottom_up#true if you want to play the bottom up mode\n",
    "        if bottom_up:\n",
    "            self.passed=False#store if the outer pendulum has managed to surpass abs(np.pi)\n",
    "            self.reset(theta1=0+add_noise*np.random.rand(),theta2=0+add_noise*np.random.rand())\n",
    "        else:\n",
    "            self.reset(add_noise=add_noise)\n",
    "            \n",
    "\n",
    "        \n",
    "    def reset(self,theta1=np.pi,theta2=np.pi,theta1_d=0,theta2_d=0,add_noise=0):\n",
    "        self.state=[theta1+add_noise*np.random.rand(),theta2+add_noise*np.random.rand(),theta1_d+add_noise*np.random.rand(),theta2_d+add_noise*np.random.rand()]\n",
    "        self.state_history=[self.state.copy()]\n",
    "        \n",
    "    #returns the current environment's observation to the agent\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    #allows the agent to query the set of actions it can execute\n",
    "    def sample_action(self):\n",
    "        #return self.state[3]+0.1*(np.random.rand()-0.5)\n",
    "        return 1.2\n",
    "\n",
    "    \n",
    "    #signals the end of the episode to the agent\n",
    "    def is_done(self):\n",
    "        return self.steps_left==0\n",
    "    \n",
    "    #central piece: handles agents action and returns reward for the action\n",
    "    def action(self,action):\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is over')\n",
    "        self.step(action)\n",
    "        delta1=np.abs(self.state[1]%(2*np.pi)-np.pi)\n",
    "        if self.bottom_up:\n",
    "            e_pot,e_kin=self.get_energy(self.state)\n",
    "            e_tot=np.sum(e_pot)+np.sum(e_kin)\n",
    "            if e_tot>1e3:\n",
    "                #energy constraint:\n",
    "                self.steps_left=0\n",
    "                return 0\n",
    "            if delta1<0.2:\n",
    "                self.passed=True\n",
    "            if self.passed:\n",
    "                delta0=np.abs(self.state[0]-np.pi)\n",
    "                if delta1<np.pi/4 and delta0<np.pi/2:\n",
    "                    self.steps_left -=1\n",
    "                    return 10-delta1**2-0.01*delta0\n",
    "                else:\n",
    "                    self.steps_left=0\n",
    "                    #self.steps_left -=1#if you want to play the game until max step reached\n",
    "                    return 0\n",
    "            else:\n",
    "                self.steps_left -=1\n",
    "                return 3-delta1**2\n",
    "        else:\n",
    "            delta0=np.abs(self.state[0]%(2*np.pi)-np.pi)\n",
    "            if delta1<np.pi/4 and delta0<np.pi/2:\n",
    "                self.steps_left -=1\n",
    "                return 2-delta1**2\n",
    "            else:\n",
    "                self.steps_left=0\n",
    "                #self.steps_left -=1#if you want to play the game until max step reached\n",
    "                return 0\n",
    "    \n",
    "    def explicite_euler(self,dt,state,F=0):\n",
    "        theta1_dd,theta2_dd=self.get_theta_dd(state,F)\n",
    "        next_state= [state[0]+dt*state[2],state[1]+dt*state[3],state[2]+dt*theta1_dd,state[3]+dt*theta2_dd]\n",
    "        return next_state\n",
    "    \n",
    "    #the decoupled equations of motion\n",
    "    def get_theta_dd(self,state,F=0):\n",
    "        theta1=state[0]\n",
    "        theta2=state[1]\n",
    "        theta1_d=state[2]\n",
    "        theta2_d=state[3]\n",
    "        #----theta1_dd-----\n",
    "        num1=-self.g*((2*self.m1+self.m2)*np.sin(theta1)+self.m2*np.sin(theta1-2*theta2))\n",
    "        num2=-2*np.sin(theta1-theta2)*self.m2*(theta2_d**2*self.l2+theta1_d**2*self.l1*np.cos(theta1-theta2))\n",
    "        num3=2*F\n",
    "        denum1=2*self.m1+self.m2-self.m2*np.cos(2*theta1-2*theta2)\n",
    "        denum=self.l1*denum1\n",
    "        theta1_dd=(num1+num2+num3)/denum\n",
    "        #----theta2_dd----\n",
    "        num1=2*np.sin(theta1-theta2)\n",
    "        num2=theta1_d**2*self.l1*(self.m1+self.m2)+self.g*(self.m1+self.m2)*np.cos(theta1)+theta2_d**2*self.l2*self.m2*np.cos(theta1-theta2)\n",
    "        num3=-2*F*np.cos(theta1-theta2)\n",
    "        denum=self.l2*denum1\n",
    "        theta2_dd=(num3+num1*num2)/denum\n",
    "        return theta1_dd,theta2_dd\n",
    "    \n",
    "    #differentail time step using explicite midpoint method\n",
    "    def step(self,F):\n",
    "        next_state=self.explicite_euler(self.dt/2,self.state,F)\n",
    "        theta1_dd,theta2_dd=self.get_theta_dd(next_state,F)\n",
    "        self.state[0]+=self.dt*next_state[2]\n",
    "        self.state[1]+=self.dt*next_state[3]\n",
    "        self.state[2]+=self.dt*theta1_dd\n",
    "        self.state[3]+=self.dt*theta2_dd\n",
    "        self.state_history.append(self.state.copy())\n",
    "        \n",
    "    \n",
    "    def render(self,img_res=1,save_path='trash_figures/inverted_double_pendulum.avi'):\n",
    "        frames_per_second=20\n",
    "        take_frame_every=int(1/(self.dt*frames_per_second))\n",
    "        frames=[]\n",
    "        h=int(img_res*200)\n",
    "        w=h\n",
    "        x0=int(w/2)\n",
    "        y0=int(h/2)\n",
    "        h_red=int(0.4*h)\n",
    "        l_tot=self.l1+self.l2\n",
    "        l1_ratio=self.l1/l_tot\n",
    "        l2_ratio=self.l2/l_tot\n",
    "        L1=l1_ratio*h_red\n",
    "        L2=l2_ratio*h_red\n",
    "        d=int(0.02*h)\n",
    "        d1=d*self.m1**(1/3)\n",
    "        d2=d*self.m2**(1/3)\n",
    "        d_4=d/4\n",
    "        #max_theta2_d=1.2*np.max(np.abs(phase_traject[:,3]))\n",
    "        for i,state_i in enumerate(self.state_history):\n",
    "            if i%5000==0:\n",
    "                print('rendering iteration: '+str(i)+'/'+str(len(self.state_history)))           \n",
    "            if i%take_frame_every==0:\n",
    "                theta1=state_i[0]\n",
    "                theta2=state_i[1]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L1*np.sin(theta1)\n",
    "                y1=y0+L1*np.cos(theta1)\n",
    "                x2=x1+L2*np.sin(theta2)\n",
    "                y2=y1+L2*np.cos(theta2)\n",
    "                #---draw the image ----\n",
    "                img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x1-d1,y1-d1),(x1+d1,y1+d1)], fill=(0,0,0), outline=None)\n",
    "                draw.line([(x1,y1),(x2,y2)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x2-d2,y2-d2),(x2+d2,y2+d2)], fill=(0,0,255), outline=None)\n",
    "                frames.append(img)\n",
    "        cv2_list=self.pil_list_to_cv2(frames)\n",
    "        self.generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "    #calculates the potential and kinetic energy of the two masses at a given state\n",
    "    def get_energy(self,state):\n",
    "        theta1=state[0]\n",
    "        theta2=state[1]\n",
    "        theta1_d=state[2]\n",
    "        theta2_d=state[3]\n",
    "        y1=self.l1*np.cos(theta1)\n",
    "        y2=y1+self.l2*np.cos(theta2)\n",
    "        e_pot=np.array([-self.m1*self.g*y1,-self.m2*self.g*y2])\n",
    "        e_kin_1=self.m1/2*(self.l1*theta1_d)**2\n",
    "        e_kin_2=(self.l1*theta1_d)**2\n",
    "        e_kin_2+=(self.l2*theta2_d)**2\n",
    "        e_kin_2+=2*self.l1*self.l2*theta1_d*theta2_d*(np.cos(theta1)*np.cos(theta2)+np.sin(theta1)*np.sin(theta2))\n",
    "        e_kin_2*=self.m2/2\n",
    "        e_kin=np.array([e_kin_1,e_kin_2])\n",
    "        return e_pot,e_kin\n",
    "    \n",
    "    #used for video converting\n",
    "    def pil_list_to_cv2(self,pil_list):\n",
    "        #converts a list of pil images to a list of cv2 images\n",
    "        png_list=[]\n",
    "        for pil_img in pil_list:\n",
    "            pil_img.save('trash_image.png',format='png')\n",
    "            png_list.append(cv2.imread('trash_image.png'))\n",
    "        os.remove('trash_image.png')\n",
    "        return png_list\n",
    "\n",
    "    def generate_video(self,cv2_list,path='car_race.avi',fps=10): \n",
    "        #makes a video from a given cv2 image list\n",
    "        if len(cv2_list)==0:\n",
    "            raise ValueError('the given png list is empty!')\n",
    "        video_name = path\n",
    "        frame=cv2_list[0] \n",
    "        # setting the frame width, height width \n",
    "        # the width, height of first image \n",
    "        height, width, layers = frame.shape   \n",
    "        video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "        # Appending the images to the video one by one \n",
    "        for cv2_image in cv2_list:  \n",
    "            video.write(cv2_image) \n",
    "        # Deallocating memories taken for window creation \n",
    "        cv2.destroyAllWindows()  \n",
    "        video.release()  # releasing the video generated \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAgent:\n",
    "    #initialize the counter for the total reward\n",
    "    def __init__(self,n_neurons,bottom_up=False):\n",
    "        self.total_reward=0.0\n",
    "        self.bottom_up=bottom_up\n",
    "        self.initialize_policy(n_neurons)\n",
    "            \n",
    "    #accepts the environment instance as an argument and allows the agents to observe and act\n",
    "    def step(self,env):\n",
    "        observation=env.get_observation()\n",
    "        #action = env.sample_action()\n",
    "        action=self.get_action(observation)\n",
    "        reward=env.action(action)\n",
    "        self.total_reward+=reward\n",
    "        \n",
    "    def reset_reward(self):\n",
    "        self.total_reward=0\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        #normalization\n",
    "        s=state.copy()\n",
    "        #if not self.bottom_up:\n",
    "            #if len(s)==2:\n",
    "                #s[0]-=np.pi\n",
    "            #elif len(s)==4:\n",
    "                #s[0]-=np.pi\n",
    "                #s[1]-=np.pi\n",
    "        with torch.no_grad():\n",
    "            F=self.policy(torch.FloatTensor(s)).item()\n",
    "            return 10*F\n",
    "        \n",
    "    def mutate(self,muatation_rate=0.1):\n",
    "        with torch.no_grad():\n",
    "            for param in self.policy.parameters():\n",
    "                param.add_(torch.randn(param.size()) * muatation_rate)\n",
    "        \n",
    "    def initialize_policy(self,n_neurons):\n",
    "        self.policy=self.get_nn(n_neurons)\n",
    "    \n",
    "    def get_nn(self,n_neurons):\n",
    "        neural_network=nn.Sequential()\n",
    "        if len(n_neurons)<2:\n",
    "            raise ValueError('n_neurons must contain at least two entries for in- and output')\n",
    "        depth=len(n_neurons)-2\n",
    "        for i in range(depth):\n",
    "            neural_network.add_module(\"layer\"+str(i),nn.Sequential(nn.Linear(n_neurons[i],n_neurons[i+1]),nn.Sigmoid()))\n",
    "        neural_network.add_module(\"layer\"+str(depth),nn.Sequential(nn.Linear(n_neurons[depth],n_neurons[depth+1])))\n",
    "        return neural_network\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for genetic algorithm\n",
    "def get_first_generation(population_size,agent,n_neurons,load_best_agent=False):\n",
    "    if load_best_agent:\n",
    "        agent=pickle.load(open('models/best_agent.pkl', \"rb\" ))\n",
    "    agent_list=[]\n",
    "    for i in range(population_size):\n",
    "        new_agent=copy.deepcopy(agent)\n",
    "        new_agent.reset_reward()\n",
    "        if load_best_agent:\n",
    "            if i>=1:\n",
    "                new_agent.mutate(muatation_rate=0.1)\n",
    "        else:\n",
    "            new_agent.initialize_policy(n_neurons)\n",
    "        agent_list.append(new_agent)\n",
    "    return agent_list\n",
    "\n",
    "def get_scores(agent_list,environment):\n",
    "    scores=[]\n",
    "    environments=[]\n",
    "    for agent in agent_list:\n",
    "        env=copy.deepcopy(environment)\n",
    "        while not env.is_done():\n",
    "            agent.step(env)\n",
    "        scores.append(agent.total_reward)\n",
    "        environments.append(env)\n",
    "    return scores,environments\n",
    "\n",
    "def get_next_generation(scores,agent_list,n_survivors,population_size,muatation_rate=0.1,store_best_agent=True):\n",
    "    np_scores=np.asarray(scores)\n",
    "    rank_idx=np.argsort(np_scores)\n",
    "    agent_elite=[]\n",
    "    for i in range(n_survivors):\n",
    "        agent_elite.append(agent_list[rank_idx[-i-1]])\n",
    "    if store_best_agent:\n",
    "        pickle.dump(agent_elite[0],open('models/best_agent.pkl', \"wb\" ))\n",
    "    new_agent_list=[]\n",
    "    for j in range(population_size):\n",
    "        new_agent=copy.deepcopy(agent_elite[j%n_survivors])\n",
    "        if j>=1:\n",
    "            new_agent.mutate(muatation_rate)\n",
    "        new_agent.reset_reward()\n",
    "        new_agent_list.append(new_agent)\n",
    "    return new_agent_list,rank_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_length(list_of_lists):\n",
    "    l=0\n",
    "    for list in list_of_lists:\n",
    "        if len(list)>l:\n",
    "            l=len(list)\n",
    "    return l\n",
    "\n",
    "#functions for video presentaiton\n",
    "def render_single_pendulum(state_history_list,dt,best_idx=None,m=1,img_res=1.5,save_path='trash_figures/inverted_pendulum_generation.avi'):\n",
    "    frames_per_second=20\n",
    "    take_frame_every=int(1/(dt*frames_per_second))\n",
    "    frames=[]\n",
    "    h=int(img_res*200)\n",
    "    w=h\n",
    "    x0=int(w/2)\n",
    "    y0=int(h/2)\n",
    "    h_red=int(0.4*h)\n",
    "    L=h_red\n",
    "    d=int(0.02*h)\n",
    "    d=d*m**(1/3)\n",
    "    if best_idx is not None:\n",
    "        state_history_list.append(state_history_list[best_idx])\n",
    "    largest_trajectory=get_largest_length(state_history_list)\n",
    "    for i in range(largest_trajectory):\n",
    "        img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        for j in range(len(state_history_list)):     \n",
    "            if i%take_frame_every==0 and len(state_history_list[j])>i:\n",
    "                if best_idx is not None:\n",
    "                    if j==len(state_history_list)-1:\n",
    "                        color=(255,0,0)\n",
    "                    else:\n",
    "                        color=(0,0,0)\n",
    "                theta=state_history_list[j][i][0]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L*np.sin(theta)\n",
    "                y1=y0+L*np.cos(theta)\n",
    "                #---draw the image ----\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=(0,0,0),width=1)\n",
    "                draw.ellipse([(x1-d,y1-d),(x1+d,y1+d)], fill=color, outline=None)\n",
    "        frames.append(img)\n",
    "    cv2_list=pil_list_to_cv2(frames)\n",
    "    generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "def render_double_pendulum(state_history_list,dt,best_idx=None,m1=1,m2=1,l1=1,l2=1,img_res=1.5,save_path='trash_figures/inverted_double_pendulum_generation.avi'):\n",
    "    frames_per_second=20\n",
    "    take_frame_every=int(1/(dt*frames_per_second))\n",
    "    frames=[]\n",
    "    h=int(img_res*200)\n",
    "    w=h\n",
    "    x0=int(w/2)\n",
    "    y0=int(h/2)\n",
    "    h_red=int(0.4*h)\n",
    "    l_tot=l1+l2\n",
    "    l1_ratio=l1/l_tot\n",
    "    l2_ratio=l2/l_tot\n",
    "    L1=l1_ratio*h_red\n",
    "    L2=l2_ratio*h_red\n",
    "    d=int(0.02*h)\n",
    "    d1=d*m1**(1/3)\n",
    "    d2=d*m2**(1/3)\n",
    "    d_4=d/4\n",
    "    if best_idx is not None:\n",
    "        state_history_list.append(state_history_list[best_idx])\n",
    "    largest_trajectory=get_largest_length(state_history_list)\n",
    "    for i in range(largest_trajectory):\n",
    "        img = Image.new(\"RGB\", (w, h), \"white\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        for j in range(len(state_history_list)):     \n",
    "            if i%take_frame_every==0 and len(state_history_list[j])>i:\n",
    "                if best_idx is not None:\n",
    "                    if j==len(state_history_list)-1:\n",
    "                        color1=(255,0,0)\n",
    "                        color2=(255,0,0)\n",
    "                    else:\n",
    "                        color1=(0,0,255)\n",
    "                        color2=(0,0,0)\n",
    "                theta=state_history_list[j][i][0]\n",
    "                theta1=state_history_list[j][i][0]\n",
    "                theta2=state_history_list[j][i][1]\n",
    "                #----transform to cartesian coordinates---\n",
    "                x1=x0+L1*np.sin(theta1)\n",
    "                y1=y0+L1*np.cos(theta1)\n",
    "                x2=x1+L2*np.sin(theta2)\n",
    "                y2=y1+L2*np.cos(theta2)\n",
    "                #---draw the image ----\n",
    "                draw.line([(x0,y0),(x1,y1)],fill=color2,width=1)\n",
    "                draw.ellipse([(x1-d1,y1-d1),(x1+d1,y1+d1)], fill=color1, outline=None)\n",
    "                draw.line([(x1,y1),(x2,y2)],fill=color2,width=1)\n",
    "                draw.ellipse([(x2-d2,y2-d2),(x2+d2,y2+d2)], fill=color1, outline=None)\n",
    "        frames.append(img)\n",
    "    cv2_list=pil_list_to_cv2(frames)\n",
    "    generate_video(cv2_list,path=save_path,fps=1000/40)\n",
    "    \n",
    "def pil_list_to_cv2(pil_list):\n",
    "    #converts a list of pil images to a list of cv2 images\n",
    "    png_list=[]\n",
    "    for pil_img in pil_list:\n",
    "        pil_img.save('trash_image.png',format='png')\n",
    "        png_list.append(cv2.imread('trash_image.png'))\n",
    "    os.remove('trash_image.png')\n",
    "    return png_list\n",
    "\n",
    "def generate_video(cv2_list,path='car_race.avi',fps=10): \n",
    "    #makes a video from a given cv2 image list\n",
    "    if len(cv2_list)==0:\n",
    "        raise ValueError('the given png list is empty!')\n",
    "    video_name = path\n",
    "    frame=cv2_list[0] \n",
    "    # setting the frame width, height width \n",
    "    # the width, height of first image \n",
    "    height, width, layers = frame.shape   \n",
    "    video = cv2.VideoWriter(video_name, 0, fps, (width, height))  \n",
    "    # Appending the images to the video one by one \n",
    "    for cv2_image in cv2_list:  \n",
    "        video.write(cv2_image) \n",
    "    # Deallocating memories taken for window creation \n",
    "    cv2.destroyAllWindows()  \n",
    "    video.release()  # releasing the video generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation: 1, best score: 157.42216298416497\n",
      "generation: 2, best score: 184.6745874705414\n",
      "generation: 3, best score: 207.5024253231064\n",
      "generation: 4, best score: 465.3568384917641\n",
      "generation: 5, best score: 465.3500179284539\n",
      "generation: 6, best score: 455.3366472036032\n",
      "generation: 7, best score: 445.48346510969355\n",
      "generation: 8, best score: 455.49379962754256\n",
      "generation: 9, best score: 465.2216215896461\n",
      "generation: 10, best score: 455.15886173209236\n",
      "generation: 11, best score: 465.52523124085536\n",
      "generation: 12, best score: 455.36485301793243\n",
      "generation: 13, best score: 465.1343227683871\n",
      "generation: 14, best score: 455.52201242040917\n",
      "generation: 15, best score: 465.28283871742775\n",
      "generation: 16, best score: 497.8898514285825\n",
      "generation: 17, best score: 271.74559230320983\n",
      "generation: 18, best score: 271.3445436182375\n",
      "generation: 19, best score: 271.17266770594193\n",
      "generation: 20, best score: 271.2135604240242\n",
      "generation: 21, best score: 295.91721453043357\n",
      "generation: 22, best score: 287.2081582207168\n",
      "generation: 23, best score: 295.5667687337945\n",
      "generation: 24, best score: 340.9919660562365\n",
      "generation: 25, best score: 347.38497434890917\n",
      "generation: 26, best score: 347.8650960029348\n",
      "generation: 27, best score: 380.43021968404264\n",
      "generation: 28, best score: 372.9840060931328\n",
      "generation: 29, best score: 572.3328229391204\n",
      "generation: 30, best score: 523.4140994124534\n"
     ]
    }
   ],
   "source": [
    "n_generations=30\n",
    "population_size=20\n",
    "n_survivors=3\n",
    "n_neurons=[4,10,1]\n",
    "muatation_rate=0.1\n",
    "add_noise=0.01\n",
    "max_steps=500\n",
    "bottom_up=True\n",
    "make_video=False\n",
    "\n",
    "agent=GeneticAgent(n_neurons,bottom_up=bottom_up)\n",
    "new_agents=get_first_generation(population_size,agent,n_neurons,load_best_agent=True)#all agents initialized independently at random\n",
    "running_score=0\n",
    "for i in range(n_generations):\n",
    "    state_history_list=[]\n",
    "    #bottom_up=0.5>np.random.rand()\n",
    "    #print('bottom up: '+str(bottom_up))\n",
    "    if n_neurons[0]==2:\n",
    "        env=EnvironmentInvertedPendulum(max_steps=max_steps,dt=0.03,m=1,add_noise=add_noise,bottom_up=bottom_up)\n",
    "    else:\n",
    "        env=EnvironmentInvertedDoublePendulum(max_steps=max_steps,dt=0.03,m1=0.5,m2=1,l1=1,l2=1,add_noise=add_noise,bottom_up=bottom_up)\n",
    "        if bottom_up:\n",
    "            env.reset(theta1=0+add_noise*np.random.rand(),theta2=0+add_noise*np.random.rand())\n",
    "    scores,environments=get_scores(new_agents,env)\n",
    "    new_agents,rank_idx=get_next_generation(scores,new_agents,n_survivors,population_size,muatation_rate,store_best_agent=True)\n",
    "    best_score=scores[rank_idx[-1]]\n",
    "    for environment in environments:\n",
    "        state_history_list.append(environment.state_history)\n",
    "    if make_video:\n",
    "        if n_neurons[0]==2:\n",
    "            render_single_pendulum(state_history_list,best_idx=rank_idx[-1],dt=0.03,m=1,img_res=1.5,save_path='inverted_pendulum/inverted_pendulum_gen='+str(i)+'.avi')\n",
    "        else:\n",
    "            render_double_pendulum(state_history_list,best_idx=rank_idx[-1],dt=0.03,m1=0.5,m2=1,l1=1,l2=1,img_res=1.5,save_path='inverted_double_pendulum/inverted_double_pendulum_gen='+str(i)+'.avi')\n",
    "\n",
    "    print('generation: '+str(i+1)+', best score: '+str(best_score))\n",
    "#---render epoch--\n",
    "#print('render environment...')\n",
    "#environments[rank_idx[-1]].render(save_path='inverted_pendulum/inverted_pendulum')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---check the energy\n",
    "#e_pot_start,e_kin_start=env.get_energy(env.state_history[0])\n",
    "#e_pot_end,e_kin_end=env.get_energy(env.state_history[-1])\n",
    "#e_start=np.sum(e_pot_start)+np.sum(e_kin_start)\n",
    "#e_end=np.sum(e_pot_end)+np.sum(e_kin_end)\n",
    "#print('starting energy: '+str(e_start))\n",
    "#print('final energy: '+str(e_end))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
